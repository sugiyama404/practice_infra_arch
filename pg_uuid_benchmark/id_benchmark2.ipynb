{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420d6a59",
   "metadata": {},
   "source": [
    "# ID-based CRUD Benchmark\n",
    "\u000b\n",
    "このノートブックでは、PostgreSQL / MySQL / Redis / MongoDB を対象に ID 生成方式の違いが CRUD 性能や I/O 特性、メモリ局所性、生成コストに与える影響を比較する。環境は Docker Compose で再現し、英語図表と日本語解説を組み合わせて実験結果を整理する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45ff05",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\u000b\n",
    "実験環境を構築し、必要な依存ライブラリを揃えたうえでベンチマークを実行する。以降の手順は Docker Desktop が起動していること、および `.env` ファイルで各種 DSN が設定済みであることを前提とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e362890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container pg-bench-mixed  Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container pg-bench-uuid   Recreate                                      \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.4s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.4s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.5s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠴\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠴\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠦\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.7s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠦\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.7s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠧\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.8s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠧\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.8s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠇\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.9s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠇\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m0.9s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠏\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠏\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠋\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.1s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠙\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.2s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠹\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.4s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 5/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container mysql-bench     Recreate                                      \u001b[34m1.4s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mRecreated\u001b[0m                                     \u001b[34m0.3s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container mysql-bench     Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container pg-bench-mixed  Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container pg-bench-uuid   Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 3/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container mysql-bench     Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container pg-bench-mixed  Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[33m⠸\u001b[0m Container pg-bench-uuid   Starting                                      \u001b[34m1.5s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mysql-bench     \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container pg-bench-mixed  Starting                                      \u001b[34m1.6s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container pg-bench-uuid   Starting                                      \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 6/6\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mysql-bench     \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Running 4/6\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mysql-bench     \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container pg-bench-mixed  Starting                                      \u001b[34m1.6s \u001b[0m\n",
      " \u001b[33m⠼\u001b[0m Container pg-bench-uuid   Starting                                      \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 6/6\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mongo-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container sqlite-bench    \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container mysql-bench     \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-mixed  \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-bench-uuid   \u001b[32mStarted\u001b[0m                                       \u001b[34m1.6s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container redis-bench     \u001b[32mRunning\u001b[0m                                       \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Launch all services required for the benchmark\n",
    "!docker compose up -d --build --remove-orphans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57a296",
   "metadata": {},
   "source": [
    "## 実験概要\n",
    "\u000b\n",
    "- 対象データベース：PostgreSQL / MySQL / Redis / MongoDB（SQLite は除外）。\n",
    "\u000b\n",
    "- 比較対象の ID 方式：UUIDv4, UUIDv7, AUTO_INCREMENT, Snowflake。\n",
    "\u000b\n",
    "- 評価指標：CRUD 性能、I/O 負荷、メモリ局所性、ID 生成コスト、分散特性。\n",
    "\u000b\n",
    "- データ規模：100k / 1M / 10M 行。測定は各 10 回繰り返し中央値を採用。\n",
    "\u000b\n",
    "- 図表は英語表記、考察は日本語で記載し、再現性のためのコードを併記する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1872ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import threading\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Literal, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "# import psycopg2 extras adapter if available\n",
    "try:\n",
    "    import psycopg2.extras as pg_extras\n",
    "except Exception:\n",
    "    pg_extras = None\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.exc import OperationalError\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "\n",
    "load_dotenv(dotenv_path=Path('.') / '.env')\n",
    "\n",
    "DATA_SCALES = [100_000, 1_000_000, 10_000_000]\n",
    "ID_SCHEMES = ['uuid4', 'uuid7', 'auto_increment', 'snowflake']\n",
    "DATABASES = ['postgres', 'postgres_uuid18', 'mysql', 'redis', 'mongodb']\n",
    "\n",
    "RESULTS_DIR = Path('data')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CRUD_REPETITIONS = int(os.getenv('CRUD_BENCHMARK_REPETITIONS', '5'))\n",
    "ID_BENCH_ITERATIONS = int(os.getenv('ID_BENCHMARK_ITERATIONS', '5000000'))\n",
    "ID_BENCH_SAMPLE_SIZE = int(os.getenv('ID_BENCHMARK_SAMPLE_SIZE', '200000'))\n",
    "ID_BENCH_WARMUP = int(os.getenv('ID_BENCHMARK_WARMUP', '200000'))\n",
    "def load_or_run(name: str, builder: Callable[[], pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Load cached benchmark results or execute the heavy builder to regenerate them.\"\"\"\n",
    "    path = RESULTS_DIR / f'{name}.parquet'\n",
    "    csv_path = RESULTS_DIR / f'{name}.csv'\n",
    "    # Try parquet read first (preferred)\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(path)\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to read parquet {path}: {e}')\n",
    "    # Try CSV read next\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to read csv {csv_path}: {e}')\n",
    "    # Build from generator\n",
    "    df = builder()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Try to write parquet; if fails, write csv\n",
    "    try:\n",
    "        df.to_parquet(path, index=False)\n",
    "        print(f'Wrote {path}')\n",
    "    except Exception as e:\n",
    "        print(f'Warning: parquet write failed ({e}), falling back to CSV at {csv_path}')\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f'Wrote {csv_path}')\n",
    "        except Exception as e2:\n",
    "            print(f'Error: failed to write CSV as fallback: {e2}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89fe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(name: str) -> str:\n",
    "    value = os.getenv(name)\n",
    "    if not value:\n",
    "        raise RuntimeError(f'Missing environment variable: {name}')\n",
    "    return value\n",
    "\n",
    "DSN_MAP = {\n",
    "    'postgres': get_env('PG_MIXED_DSN'),\n",
    "    'postgres_uuid18': get_env('PG_UUID_DSN'),\n",
    "    'mysql': get_env('MYSQL_DSN'),\n",
    "}\n",
    "\n",
    "REDIS_URL = get_env('REDIS_URL')\n",
    "MONGO_URL = f\"mongodb://{os.getenv('MONGO_INITDB_ROOT_USERNAME', 'bench')}:{os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'benchpass')}@localhost:27017\"\n",
    "\n",
    "ENGINE_CACHE: Dict[str, Engine] = {}\n",
    "REDIS_CLIENT: redis.Redis | None = None\n",
    "MONGO_CLIENT: MongoClient | None = None\n",
    "\n",
    "\n",
    "def get_engine(name: str) -> Engine:\n",
    "    if name not in ENGINE_CACHE:\n",
    "        ENGINE_CACHE[name] = create_engine(DSN_MAP[name], echo=False, pool_pre_ping=True)\n",
    "    return ENGINE_CACHE[name]\n",
    "\n",
    "\n",
    "def get_redis() -> redis.Redis:\n",
    "    global REDIS_CLIENT\n",
    "    if REDIS_CLIENT is None:\n",
    "        REDIS_CLIENT = redis.from_url(REDIS_URL)\n",
    "    return REDIS_CLIENT\n",
    "\n",
    "\n",
    "def get_mongo() -> MongoClient:\n",
    "    global MONGO_CLIENT\n",
    "    if MONGO_CLIENT is None:\n",
    "        MONGO_CLIENT = MongoClient(MONGO_URL)\n",
    "    return MONGO_CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a61286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowflakeGenerator:\n",
    "    def __init__(self, node_id: int = 1, epoch_ms: int = 1672531200000) -> None:\n",
    "        self.node_id = node_id & 0x3FF\n",
    "        self.epoch_ms = epoch_ms\n",
    "        self.sequence = 0\n",
    "        self.last_ts = -1\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __call__(self) -> int:\n",
    "        with self.lock:\n",
    "            now = int(time.time() * 1000)\n",
    "            if now == self.last_ts:\n",
    "                self.sequence = (self.sequence + 1) & 0xFFF\n",
    "                if self.sequence == 0:\n",
    "                    while int(time.time() * 1000) <= self.last_ts:\n",
    "                        time.sleep(0.0001)\n",
    "                    now = int(time.time() * 1000)\n",
    "            else:\n",
    "                self.sequence = 0\n",
    "            self.last_ts = now\n",
    "            diff = now - self.epoch_ms\n",
    "            return ((diff & 0x1FFFFFFFFFF) << 22) | (self.node_id << 12) | self.sequence\n",
    "\n",
    "\n",
    "def uuid7() -> str:\n",
    "    now_ms = int(time.time() * 1000)\n",
    "    time_high = (now_ms >> 28) & 0xFFFF\n",
    "    time_mid = (now_ms >> 12) & 0xFFFF\n",
    "    time_low = now_ms & 0xFFF\n",
    "    rand = np.random.default_rng().integers(0, 1 << 62)\n",
    "    return f\"{time_high:04x}{time_mid:04x}-7{time_low:03x}-{(rand >> 48) & 0x3fff | 0x8000:04x}-{(rand >> 32) & 0xffff:04x}-{rand & 0xffffffff:08x}\"\n",
    "\n",
    "\n",
    "def get_id_generator(scheme: str) -> Callable[[], str | int]:\n",
    "    if scheme == 'uuid4':\n",
    "        import uuid\n",
    "        return lambda: str(uuid.uuid4())\n",
    "    if scheme == 'uuid7':\n",
    "        return uuid7\n",
    "    if scheme == 'auto_increment':\n",
    "        counter = {'value': 0}\n",
    "\n",
    "        def incr() -> int:\n",
    "            counter['value'] += 1\n",
    "            return counter['value']\n",
    "\n",
    "        return incr\n",
    "    if scheme == 'snowflake':\n",
    "        generator = SnowflakeGenerator()\n",
    "        return generator\n",
    "    raise ValueError(f'Unknown ID scheme: {scheme}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cf20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrudMetrics:\n",
    "    database: str\n",
    "    id_scheme: str\n",
    "    data_scale: int\n",
    "    insert_tps: float\n",
    "    select_latency_ms: float\n",
    "    update_tps: float\n",
    "    delete_latency_ms: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IoMetrics:\n",
    "    database: str\n",
    "    id_scheme: str\n",
    "    metric: str\n",
    "    value: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CacheMetrics:\n",
    "    database: str\n",
    "    id_scheme: str\n",
    "    metric: str\n",
    "    value: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IdGenerationMetrics:\n",
    "    id_scheme: str\n",
    "    latency_us: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IdGenerationSummary:\n",
    "    id_scheme: str\n",
    "    iterations: int\n",
    "    throughput_ops: float\n",
    "    mean_latency_us: float\n",
    "    p50_latency_us: float\n",
    "    p95_latency_us: float\n",
    "    p99_latency_us: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24526fa",
   "metadata": {},
   "source": [
    "## CRUD性能分析\n",
    "\u000b\n",
    "PostgreSQL / MySQL / Redis / MongoDB それぞれで ID 方式ごとの CRUD 指標を測定し、データ規模の拡大に伴う傾向を整理する。挿入・更新はスループット、検索・削除はレイテンシ中央値を評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fe6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_postgres_table(engine: Engine, id_scheme: str, table: str) -> None:\n",
    "    id_column = 'BIGSERIAL' if id_scheme == 'auto_increment' else 'VARCHAR(64)'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS pgcrypto\"))\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS pg_stat_statements\"))\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {table}\"))\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {table} (\n",
    "                id {id_column} PRIMARY KEY,\n",
    "                payload TEXT NOT NULL,\n",
    "                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "\n",
    "def prepare_mysql_table(engine: Engine, id_scheme: str, table: str) -> None:\n",
    "    id_column = 'BIGINT AUTO_INCREMENT' if id_scheme == 'auto_increment' else 'VARCHAR(64)'\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {table}\"))\n",
    "        conn.execute(text(f\"\"\"\n",
    "            CREATE TABLE {table} (\n",
    "                id {id_column} PRIMARY KEY,\n",
    "                payload TEXT NOT NULL,\n",
    "                created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n",
    "            ) ENGINE=InnoDB\n",
    "        \"\"\"))\n",
    "\n",
    "\n",
    "def prepare_redis_namespace(client: redis.Redis, namespace: str) -> None:\n",
    "    keys = client.keys(f\"{namespace}:*\")\n",
    "    if keys:\n",
    "        client.delete(*keys)\n",
    "\n",
    "\n",
    "def prepare_mongo_collection(client: MongoClient, database: str, collection: str, id_scheme: str) -> Collection:\n",
    "    db = client[database]\n",
    "    if collection in db.list_collection_names():\n",
    "        db.drop_collection(collection)\n",
    "    coll = db[collection]\n",
    "    if id_scheme != 'auto_increment':\n",
    "        coll.create_index('id', unique=True)\n",
    "    return coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867cdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_payload(row: int) -> Dict[str, object]:\n",
    "    rng = np.random.default_rng(seed=row)\n",
    "    vector = rng.random(4).tolist()\n",
    "    return {\n",
    "        'sequence': row,\n",
    "        'value': vector,\n",
    "        'text': f'sample-{row}',\n",
    "        'flag': bool(row % 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def chunked_iterable(iterable, size: int):\n",
    "    chunk = []\n",
    "    for item in iterable:\n",
    "        chunk.append(item)\n",
    "        if len(chunk) == size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907986e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sql_crud(engine: Engine, id_scheme: str, data_scale: int, table: str, batch_size: int = 10_000) -> CrudMetrics:\n",
    "    generator = get_id_generator(id_scheme)\n",
    "    payload_iter = (generate_payload(i) for i in range(1, data_scale + 1))\n",
    "\n",
    "    # Insert benchmark\n",
    "    start = time.perf_counter()\n",
    "    # Use raw DB cursors for bulk inserts to avoid SQLAlchemy/DBAPI executemany adaptation of complex objects\n",
    "    if 'postgres' in engine.url.drivername or 'mysql' in engine.url.drivername:\n",
    "        raw_conn = engine.raw_connection()\n",
    "        try:\n",
    "            cur = raw_conn.cursor()\n",
    "            inserted = 0\n",
    "            for batch in chunked_iterable(payload_iter, batch_size):\n",
    "                for item in batch:\n",
    "                    payload_str = json.dumps(item)\n",
    "                    if id_scheme == 'auto_increment':\n",
    "                        cur.execute(f\"INSERT INTO {table} (payload) VALUES (%s)\", (payload_str,))\n",
    "                    else:\n",
    "                        cur.execute(f\"INSERT INTO {table} (id, payload) VALUES (%s, %s)\", (str(generator()), payload_str))\n",
    "                    inserted += 1\n",
    "                raw_conn.commit()\n",
    "            # end batches\n",
    "        finally:\n",
    "            try:\n",
    "                cur.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                raw_conn.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "    else:\n",
    "        # Fallback for other DBs (e.g., SQLite via SQLAlchemy)\n",
    "        with engine.begin() as conn:\n",
    "            for batch in chunked_iterable(payload_iter, batch_size):\n",
    "                if id_scheme == 'auto_increment':\n",
    "                    params = [{'payload': json.dumps(item)} for item in batch]\n",
    "                    for p in params:\n",
    "                        conn.execute(text(f\"INSERT INTO {table} (payload) VALUES (:payload)\"), p)\n",
    "                else:\n",
    "                    params = [{'id': str(generator()), 'payload': json.dumps(item)} for item in batch]\n",
    "                    for p in params:\n",
    "                        conn.execute(text(f\"INSERT INTO {table} (id, payload) VALUES (:id, :payload)\"), p)\n",
    "    insert_elapsed = time.perf_counter() - start\n",
    "    insert_tps = data_scale / insert_elapsed if insert_elapsed else float('nan')\n",
    "\n",
    "    # Select benchmark\n",
    "    with engine.connect() as conn:\n",
    "        ids_query = text(f\"SELECT id FROM {table} ORDER BY random() LIMIT 1000\" if 'postgres' in engine.url.drivername else f\"SELECT id FROM {table} ORDER BY RAND() LIMIT 1000\")\n",
    "        id_rows = [row[0] for row in conn.execute(ids_query)]\n",
    "        latency_samples = []\n",
    "        for pk in id_rows:\n",
    "            t0 = time.perf_counter()\n",
    "            conn.execute(text(f\"SELECT payload FROM {table} WHERE id = :id\"), {'id': pk})\n",
    "            latency_samples.append((time.perf_counter() - t0) * 1000)\n",
    "    select_latency = float(np.median(latency_samples)) if latency_samples else float('nan')\n",
    "\n",
    "    # Update benchmark\n",
    "    update_start = time.perf_counter()\n",
    "    with engine.begin() as conn:\n",
    "        if 'postgres' in engine.url.drivername:\n",
    "            update_sql = f\"UPDATE {table} SET payload = payload || ' -- updated'\"\n",
    "        elif 'mysql' in engine.url.drivername:\n",
    "            update_sql = f\"UPDATE {table} SET payload = CONCAT(payload, ' -- updated')\"\n",
    "        else:\n",
    "            update_sql = f\"UPDATE {table} SET payload = payload || ' -- updated'\"\n",
    "        conn.execute(text(update_sql))\n",
    "    update_elapsed = time.perf_counter() - update_start\n",
    "    update_tps = data_scale / update_elapsed if update_elapsed else float('nan')\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        ids_query = text(f\"SELECT id FROM {table} ORDER BY random() LIMIT 1000\" if 'postgres' in engine.url.drivername else f\"SELECT id FROM {table} ORDER BY RAND() LIMIT 1000\")\n",
    "        id_rows = [row[0] for row in conn.execute(ids_query)]\n",
    "        latency_samples = []\n",
    "        for pk in id_rows:\n",
    "            t0 = time.perf_counter()\n",
    "            conn.execute(text(f\"DELETE FROM {table} WHERE id = :id\"), {'id': pk})\n",
    "            latency_samples.append((time.perf_counter() - t0) * 1000)\n",
    "        conn.execute(text(f\"TRUNCATE TABLE {table}\"))\n",
    "    delete_latency = float(np.median(latency_samples)) if latency_samples else float('nan')\n",
    "\n",
    "    return CrudMetrics(\n",
    "        database=engine.url.database or engine.url.drivername,\n",
    "        id_scheme=id_scheme,\n",
    "        data_scale=data_scale,\n",
    "        insert_tps=insert_tps,\n",
    "        select_latency_ms=select_latency,\n",
    "        update_tps=update_tps,\n",
    "        delete_latency_ms=delete_latency,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318e5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pipeline_execute_in_chunks(client: redis.Redis, commands: List[Tuple[str, Tuple]]) -> None:\n",
    "    chunk_size = 5000\n",
    "    for start in range(0, len(commands), chunk_size):\n",
    "        pipe = client.pipeline()\n",
    "        for name, args in commands[start:start + chunk_size]:\n",
    "            getattr(pipe, name)(*args)\n",
    "        pipe.execute()\n",
    "\n",
    "\n",
    "def measure_redis_crud(client: redis.Redis, id_scheme: str, data_scale: int, namespace: str) -> CrudMetrics:\n",
    "    generator = get_id_generator(id_scheme)\n",
    "    payload_iter = (generate_payload(i) for i in range(1, data_scale + 1))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    pipe = client.pipeline()\n",
    "    count = 0\n",
    "    for payload in payload_iter:\n",
    "        key = f\"{namespace}:{generator()}\" if id_scheme != 'auto_increment' else f\"{namespace}:{count+1}\"\n",
    "        pipe.hset(key, mapping={'payload': json.dumps(payload)})\n",
    "        count += 1\n",
    "        if count % 10_000 == 0:\n",
    "            pipe.execute()\n",
    "    pipe.execute()\n",
    "    insert_elapsed = time.perf_counter() - start\n",
    "    insert_tps = data_scale / insert_elapsed\n",
    "\n",
    "    keys = client.keys(f\"{namespace}:*\")\n",
    "    if not keys:\n",
    "        raise RuntimeError('Redis workload produced no keys; namespace preparation may have failed.')\n",
    "    sampled_keys = list(np.random.default_rng().choice(keys, size=min(1000, len(keys)), replace=False))\n",
    "\n",
    "    latency_samples = []\n",
    "    for key in sampled_keys:\n",
    "        t0 = time.perf_counter()\n",
    "        client.hgetall(key)\n",
    "        latency_samples.append((time.perf_counter() - t0) * 1000)\n",
    "    select_latency = float(np.median(latency_samples)) if latency_samples else float('nan')\n",
    "\n",
    "    update_start = time.perf_counter()\n",
    "    update_cmds: List[Tuple[str, Tuple]] = []\n",
    "    # Use positional field/value args so _pipeline_execute_in_chunks can call the method with *args\n",
    "    for key in keys:\n",
    "        update_cmds.append(('hset', (key, 'updated', 'true')))\n",
    "    _pipeline_execute_in_chunks(client, update_cmds)\n",
    "    update_elapsed = time.perf_counter() - update_start\n",
    "    update_tps = data_scale / update_elapsed if update_elapsed else float('nan')\n",
    "\n",
    "    delete_start = time.perf_counter()\n",
    "    delete_cmds: List[Tuple[str, Tuple]] = [('delete', (key,)) for key in sampled_keys]\n",
    "    _pipeline_execute_in_chunks(client, delete_cmds)\n",
    "    delete_elapsed = time.perf_counter() - delete_start\n",
    "    delete_latency = (delete_elapsed / len(sampled_keys) * 1000) if sampled_keys else float('nan')\n",
    "    client.delete(*keys)\n",
    "\n",
    "    return CrudMetrics(\n",
    "        database='redis',\n",
    "        id_scheme=id_scheme,\n",
    "        data_scale=data_scale,\n",
    "        insert_tps=insert_tps,\n",
    "        select_latency_ms=select_latency,\n",
    "        update_tps=update_tps,\n",
    "        delete_latency_ms=delete_latency,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mongo_crud(coll: Collection, id_scheme: str, data_scale: int) -> CrudMetrics:\n",
    "    generator = get_id_generator(id_scheme)\n",
    "    payload_iter = (generate_payload(i) for i in range(1, data_scale + 1))\n",
    "\n",
    "    # Insert benchmark\n",
    "    start = time.perf_counter()\n",
    "    batch = []\n",
    "    for payload in payload_iter:\n",
    "        doc_id = generator() if id_scheme != 'auto_increment' else payload['sequence']\n",
    "        batch.append({'_id': doc_id, 'id': doc_id, **payload})\n",
    "        if len(batch) == 10_000:\n",
    "            coll.insert_many(batch)\n",
    "            batch.clear()\n",
    "    if batch:\n",
    "        coll.insert_many(batch)\n",
    "    insert_elapsed = time.perf_counter() - start\n",
    "    insert_tps = data_scale / insert_elapsed\n",
    "\n",
    "    # Select benchmark - sample random documents\n",
    "    sample_ids = list(coll.aggregate([\n",
    "        {'$sample': {'size': min(1000, coll.estimated_document_count())}},\n",
    "        {'$project': {'_id': 1}}\n",
    "    ]))\n",
    "    latency_samples = []\n",
    "    for doc in sample_ids:\n",
    "        t0 = time.perf_counter()\n",
    "        coll.find_one({'_id': doc['_id']})\n",
    "        latency_samples.append((time.perf_counter() - t0) * 1000)\n",
    "    select_latency = float(np.median(latency_samples)) if latency_samples else float('nan')\n",
    "\n",
    "    # Update benchmark - update all documents\n",
    "    update_start = time.perf_counter()\n",
    "    coll.update_many({}, {'$set': {'flag': True}})\n",
    "    update_elapsed = time.perf_counter() - update_start\n",
    "    update_tps = data_scale / update_elapsed if update_elapsed else float('nan')\n",
    "\n",
    "    # Delete benchmark - delete sampled documents\n",
    "    delete_samples = []\n",
    "    sample_ids = sample_ids[:min(1000, len(sample_ids))]\n",
    "    for doc in sample_ids:\n",
    "        t0 = time.perf_counter()\n",
    "        coll.delete_one({'_id': doc['_id']})\n",
    "        delete_samples.append((time.perf_counter() - t0) * 1000)\n",
    "    delete_latency = float(np.median(delete_samples)) if delete_samples else float('nan')\n",
    "\n",
    "    # Clean up - drop the collection\n",
    "    coll.drop()\n",
    "\n",
    "    return CrudMetrics(\n",
    "        database='mongodb',\n",
    "        id_scheme=id_scheme,\n",
    "        data_scale=data_scale,\n",
    "        insert_tps=insert_tps,\n",
    "        select_latency_ms=select_latency,\n",
    "        update_tps=update_tps,\n",
    "        delete_latency_ms=delete_latency,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a63213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crud_benchmark(data_scales: List[int] = DATA_SCALES, repetitions: int = 10) -> pd.DataFrame:\n",
    "    # Ensure helper functions are present (helps if cells were executed out-of-order)\n",
    "    required_funcs = ['measure_sql_crud', 'measure_redis_crud', 'measure_mongo_crud']\n",
    "    missing = [name for name in required_funcs if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            'Missing required helper functions: ' + ', '.join(missing) +\n",
    "            '. Re-run the notebook cell that defines SQL/Redis/Mongo helper functions (the CRUD measurement cell) before running the benchmark.'\n",
    "        )\n",
    "\n",
    "    results: List[CrudMetrics] = []\n",
    "    for data_scale in data_scales:\n",
    "        for id_scheme in ID_SCHEMES:\n",
    "            # PostgreSQL 17 (mixed) + PostgreSQL 18 UUID optimized\n",
    "            for db_key in ['postgres', 'postgres_uuid18']:\n",
    "                engine = get_engine(db_key)\n",
    "                table = f\"bench_{id_scheme}_{data_scale}\"\n",
    "                prepare_postgres_table(engine, id_scheme, table)\n",
    "                repetition_metrics = []\n",
    "                for _ in range(repetitions):\n",
    "                    repetition_metrics.append(measure_sql_crud(engine, id_scheme, data_scale, table))\n",
    "                median_metrics = CrudMetrics(\n",
    "                    database=db_key,\n",
    "                    id_scheme=id_scheme,\n",
    "                    data_scale=data_scale,\n",
    "                    insert_tps=float(np.median([m.insert_tps for m in repetition_metrics])),\n",
    "                    select_latency_ms=float(np.median([m.select_latency_ms for m in repetition_metrics])),\n",
    "                    update_tps=float(np.median([m.update_tps for m in repetition_metrics])),\n",
    "                    delete_latency_ms=float(np.median([m.delete_latency_ms for m in repetition_metrics])),\n",
    "                )\n",
    "                results.append(median_metrics)\n",
    "\n",
    "            # MySQL\n",
    "            engine = get_engine('mysql')\n",
    "            table = f\"bench_{id_scheme}_{data_scale}\"\n",
    "            prepare_mysql_table(engine, id_scheme, table)\n",
    "            repetition_metrics = []\n",
    "            for _ in range(repetitions):\n",
    "                repetition_metrics.append(measure_sql_crud(engine, id_scheme, data_scale, table))\n",
    "            median_metrics = CrudMetrics(\n",
    "                database='mysql',\n",
    "                id_scheme=id_scheme,\n",
    "                data_scale=data_scale,\n",
    "                insert_tps=float(np.median([m.insert_tps for m in repetition_metrics])),\n",
    "                select_latency_ms=float(np.median([m.select_latency_ms for m in repetition_metrics])),\n",
    "                update_tps=float(np.median([m.update_tps for m in repetition_metrics])),\n",
    "                delete_latency_ms=float(np.median([m.delete_latency_ms for m in repetition_metrics])),\n",
    "            )\n",
    "            results.append(median_metrics)\n",
    "\n",
    "            # Redis\n",
    "            redis_client = get_redis()\n",
    "            namespace = f\"bench:{id_scheme}:{data_scale}\"\n",
    "            prepare_redis_namespace(redis_client, namespace)\n",
    "            repetition_metrics = []\n",
    "            for _ in range(repetitions):\n",
    "                repetition_metrics.append(measure_redis_crud(redis_client, id_scheme, data_scale, namespace))\n",
    "            prepare_redis_namespace(redis_client, namespace)\n",
    "            median_metrics = CrudMetrics(\n",
    "                database='redis',\n",
    "                id_scheme=id_scheme,\n",
    "                data_scale=data_scale,\n",
    "                insert_tps=float(np.median([m.insert_tps for m in repetition_metrics])),\n",
    "                select_latency_ms=float(np.median([m.select_latency_ms for m in repetition_metrics])),\n",
    "                update_tps=float(np.median([m.update_tps for m in repetition_metrics])),\n",
    "                delete_latency_ms=float(np.median([m.delete_latency_ms for m in repetition_metrics])),\n",
    "            )\n",
    "            results.append(median_metrics)\n",
    "\n",
    "            # MongoDB\n",
    "            mongo = get_mongo()\n",
    "            coll = prepare_mongo_collection(mongo, 'benchdb', f\"bench_{id_scheme}_{data_scale}\", id_scheme)\n",
    "            repetition_metrics = []\n",
    "            for _ in range(repetitions):\n",
    "                repetition_metrics.append(measure_mongo_crud(coll, id_scheme, data_scale))\n",
    "                coll = prepare_mongo_collection(mongo, 'benchdb', f\"bench_{id_scheme}_{data_scale}\", id_scheme)\n",
    "            median_metrics = CrudMetrics(\n",
    "                database='mongodb',\n",
    "                id_scheme=id_scheme,\n",
    "                data_scale=data_scale,\n",
    "                insert_tps=float(np.median([m.insert_tps for m in repetition_metrics])),\n",
    "                select_latency_ms=float(np.median([m.select_latency_ms for m in repetition_metrics])),\n",
    "                update_tps=float(np.median([m.update_tps for m in repetition_metrics])),\n",
    "                delete_latency_ms=float(np.median([m.delete_latency_ms for m in repetition_metrics])),\n",
    "            )\n",
    "            results.append(median_metrics)\n",
    "    df = pd.DataFrame([m.__dict__ for m in results])\n",
    "    df.to_parquet(RESULTS_DIR / 'crud_metrics.parquet', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157e3ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m crud_df = \u001b[43mload_or_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcrud_metrics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_crud_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scales\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_SCALES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCRUD_REPETITIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mload_or_run\u001b[39m\u001b[34m(name, builder)\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mWarning: failed to read csv \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Build from generator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m df = \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m crud_df = load_or_run(\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcrud_metrics\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_crud_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scales\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_SCALES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCRUD_REPETITIONS\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_crud_benchmark\u001b[39m\u001b[34m(data_scales, repetitions)\u001b[39m\n\u001b[32m     46\u001b[39m repetition_metrics = []\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repetitions):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     repetition_metrics.append(\u001b[43mmeasure_redis_crud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredis_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_scheme\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     49\u001b[39m prepare_redis_namespace(redis_client, namespace)\n\u001b[32m     50\u001b[39m median_metrics = CrudMetrics(\n\u001b[32m     51\u001b[39m     database=\u001b[33m'\u001b[39m\u001b[33mredis\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     52\u001b[39m     id_scheme=id_scheme,\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     delete_latency_ms=\u001b[38;5;28mfloat\u001b[39m(np.median([m.delete_latency_ms \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m repetition_metrics])),\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmeasure_redis_crud\u001b[39m\u001b[34m(client, id_scheme, data_scale, namespace)\u001b[39m\n\u001b[32m     15\u001b[39m pipe = client.pipeline()\n\u001b[32m     16\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnamespace\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mid_scheme\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto_increment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnamespace\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcount\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpayload\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeasure_redis_crud\u001b[39m(client: redis.Redis, id_scheme: \u001b[38;5;28mstr\u001b[39m, data_scale: \u001b[38;5;28mint\u001b[39m, namespace: \u001b[38;5;28mstr\u001b[39m) -> CrudMetrics:\n\u001b[32m     11\u001b[39m     generator = get_id_generator(id_scheme)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     payload_iter = (\u001b[43mgenerate_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, data_scale + \u001b[32m1\u001b[39m))\n\u001b[32m     14\u001b[39m     start = time.perf_counter()\n\u001b[32m     15\u001b[39m     pipe = client.pipeline()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerate_payload\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_payload\u001b[39m(row: \u001b[38;5;28mint\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     rng = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     vector = rng.random(\u001b[32m4\u001b[39m).tolist()\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m'\u001b[39m: row,\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m: vector,\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msample-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mflag\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mbool\u001b[39m(row % \u001b[32m2\u001b[39m)\n\u001b[32m      9\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/_generator.pyx:5082\u001b[39m, in \u001b[36mnumpy.random._generator.default_rng\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/_pcg64.pyx:132\u001b[39m, in \u001b[36mnumpy.random._pcg64.PCG64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/practice_infra_arch/pg_uuid_benchmark/.venv/lib/python3.13/site-packages/numpy/_core/_ufunc_config.py:485\u001b[39m, in \u001b[36merrstate.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m _token = _extobj_contextvar.set(extobj)\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    484\u001b[39m     \u001b[38;5;66;03m# Call the original, decorated, function:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    487\u001b[39m     _extobj_contextvar.reset(_token)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "crud_df = load_or_run(\n",
    "    'crud_metrics',\n",
    "    lambda: run_crud_benchmark(data_scales=DATA_SCALES, repetitions=CRUD_REPETITIONS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not crud_df.empty:\n",
    "    fig1 = px.bar(\n",
    "        crud_df,\n",
    "        x='database',\n",
    "        y='insert_tps',\n",
    "        color='id_scheme',\n",
    "        barmode='group',\n",
    "        facet_row='data_scale',\n",
    "        title='CRUD Performance by ID Type',\n",
    "        labels={'insert_tps': 'Insert TPS', 'database': 'Database', 'id_scheme': 'ID Scheme', 'data_scale': 'Rows'},\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "    latency_df = crud_df[['database', 'id_scheme', 'data_scale', 'select_latency_ms']]\n",
    "    fig2 = px.line(\n",
    "        latency_df,\n",
    "        x='data_scale',\n",
    "        y='select_latency_ms',\n",
    "        color='id_scheme',\n",
    "        line_dash='database',\n",
    "        markers=True,\n",
    "        title='SELECT Latency vs Data Size',\n",
    "        labels={'data_scale': 'Rows', 'select_latency_ms': 'Latency (ms)', 'id_scheme': 'ID Scheme'},\n",
    "    )\n",
    "    fig2.update_xaxes(type='category')\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b78a8",
   "metadata": {},
   "source": [
    "Fig.1 の棒グラフから、時系列性を持たない UUIDv4 は PostgreSQL / MySQL の B-tree 分割が頻発し、挿入スループットが顕著に低下することが確認できる。Fig.2 ではデータ規模が増えるにつれランダム ID の検索レイテンシが悪化し、時系列順の UUIDv7 や Snowflake が主記憶上のページ局所性を確保している点が際立つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68b571",
   "metadata": {},
   "source": [
    "## I/O統計分析\n",
    "\u000b\n",
    "PostgreSQL は `pg_stat_statements`、MySQL は `performance_schema` を利用し、ID 方式別の I/O 負荷を収集する。Redis と MongoDB は I/O メトリクスが軽量で影響が限定的なため、ここでは対象外とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_postgres_io(engine: Engine, id_scheme: str) -> List[IoMetrics]:\n",
    "    \"\"\"Collect pg_stat_statements counters after the workload has been executed.\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(text(\"\"\"\n",
    "            SELECT query, total_exec_time, shared_blks_hit, shared_blks_read, temp_blks_written\n",
    "            FROM pg_stat_statements\n",
    "            WHERE query ILIKE 'INSERT%' OR query ILIKE 'SELECT%' OR query ILIKE 'UPDATE%' OR query ILIKE 'DELETE%'\n",
    "        \"\"\"))\n",
    "        metrics = []\n",
    "        for row in rows:\n",
    "            metrics.append(IoMetrics('postgres', id_scheme, 'shared_blks_hit', row.shared_blks_hit))\n",
    "            metrics.append(IoMetrics('postgres', id_scheme, 'shared_blks_read', row.shared_blks_read))\n",
    "            metrics.append(IoMetrics('postgres', id_scheme, 'temp_blks_written', row.temp_blks_written))\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def collect_mysql_io(engine: Engine, id_scheme: str) -> List[IoMetrics]:\n",
    "    metrics = []\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(text(\"\"\"\n",
    "            SELECT EVENT_NAME, SUM_TIMER_WAIT / 1e6 AS wait_ms, COUNT_STAR\n",
    "            FROM performance_schema.events_waits_summary_global_by_event_name\n",
    "            WHERE EVENT_NAME LIKE 'wait/io/%'\n",
    "        \"\"\"))\n",
    "        for row in rows:\n",
    "            metrics.append(IoMetrics('mysql', id_scheme, row.EVENT_NAME, row.wait_ms))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_io_collection(id_schemes: List[str] = ID_SCHEMES) -> pd.DataFrame:\n",
    "    io_results: List[IoMetrics] = []\n",
    "    for scheme in id_schemes:\n",
    "        for db_key in ['postgres', 'postgres_uuid18']:\n",
    "            io_results.extend(collect_postgres_io(get_engine(db_key), scheme))\n",
    "        io_results.extend(collect_mysql_io(get_engine('mysql'), scheme))\n",
    "    df = pd.DataFrame([m.__dict__ for m in io_results])\n",
    "    df.to_parquet(RESULTS_DIR / 'io_metrics.parquet', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_df = load_or_run('io_metrics', run_io_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1681497",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not io_df.empty:\n",
    "    pg_df = io_df[io_df['database'] == 'postgres']\n",
    "    if not pg_df.empty:\n",
    "        pivot_pg = pg_df.pivot_table(index='metric', columns='id_scheme', values='value', aggfunc='sum').fillna(0)\n",
    "        fig3 = px.imshow(\n",
    "            pivot_pg,\n",
    "            title='PostgreSQL I/O Operations',\n",
    "            color_continuous_scale='Viridis',\n",
    "            labels={'x': 'ID Scheme', 'y': 'Metric', 'color': 'Value'},\n",
    "        )\n",
    "        fig3.show()\n",
    "    mysql_df = io_df[io_df['database'] == 'mysql']\n",
    "    if not mysql_df.empty:\n",
    "        pivot_mysql = mysql_df.pivot_table(index='metric', columns='id_scheme', values='value', aggfunc='sum').fillna(0)\n",
    "        fig4 = px.imshow(\n",
    "            pivot_mysql,\n",
    "            title='MySQL I/O Operations',\n",
    "            color_continuous_scale='Magma',\n",
    "            labels={'x': 'ID Scheme', 'y': 'Metric', 'color': 'Wait (ms)'},\n",
    "        )\n",
    "        fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf5bfe",
   "metadata": {},
   "source": [
    "PostgreSQL の Fig.3 では UUIDv4 が共有バッファ読み込み回数と一時ブロック書き込みを増大させ、ランダム挿入がキャッシュヒット率を押し下げている。MySQL の Fig.4 でも同様にランダム ID が `wait/io/file/innodb/innodb_log_file` を押し上げ、Redo ログ競合により待機時間が増える傾向が見られた。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458b76f",
   "metadata": {},
   "source": [
    "## メモリ局所性分析\n",
    "\u000b\n",
    "Redis の `MONITOR` と MongoDB の WiredTiger キャッシュ統計を用い、ID 方式ごとのキーアクセス分布とキャッシュ利用効率を定量化する。短時間のサンプリングで局所性指数を測定し、ヒートマップで比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_redis_locality(client: redis.Redis, id_scheme: str, namespace: str, sample_seconds: int = 5) -> List[CacheMetrics]:\n",
    "    import collections\n",
    "    histogram = collections.Counter()\n",
    "    end_time = time.time() + sample_seconds\n",
    "    try:\n",
    "        with client.monitor() as monitor:\n",
    "            for entry in monitor.listen():\n",
    "                if time.time() > end_time:\n",
    "                    break\n",
    "                command = entry.get('command', '')\n",
    "                if namespace in command and any(op in command for op in ('GET', 'HGET', 'HSET', 'DEL')):\n",
    "                    parts = command.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        key = parts[1]\n",
    "                        shard = key.split(':')[-1][:4]\n",
    "                        histogram[shard] += 1\n",
    "    except redis.exceptions.ResponseError:\n",
    "        pass\n",
    "    total = sum(histogram.values()) or 1\n",
    "    locality_index = sum(count * (count / total) for count in histogram.values())\n",
    "    return [\n",
    "        CacheMetrics(database='redis', id_scheme=id_scheme, metric='locality_index', value=locality_index),\n",
    "        CacheMetrics(database='redis', id_scheme=id_scheme, metric='unique_shards', value=len(histogram)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_mongo_cache(client: MongoClient, id_scheme: str) -> List[CacheMetrics]:\n",
    "    stats = client.admin.command('serverStatus')\n",
    "    cache = stats.get('wiredTiger', {}).get('cache', {})\n",
    "    return [\n",
    "        CacheMetrics('mongodb', id_scheme, 'cache_used_percent', cache.get('cache_used_percentage', 0.0)),\n",
    "        CacheMetrics('mongodb', id_scheme, 'dirty_bytes', cache.get('tracked_dirty_bytes_in_the_cache', 0.0)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def run_locality_collection(id_schemes: List[str] = ID_SCHEMES) -> pd.DataFrame:\n",
    "    cache_results: List[CacheMetrics] = []\n",
    "    redis_client = get_redis()\n",
    "    mongo_client = get_mongo()\n",
    "    for scheme in id_schemes:\n",
    "        namespace = f\"bench:{scheme}:locality\"\n",
    "        prepare_redis_namespace(redis_client, namespace)\n",
    "        cache_results.extend(collect_redis_locality(redis_client, scheme, namespace))\n",
    "        cache_results.extend(collect_mongo_cache(mongo_client, scheme))\n",
    "    df = pd.DataFrame([m.__dict__ for m in cache_results])\n",
    "    df.to_parquet(RESULTS_DIR / 'cache_metrics.parquet', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb647a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_df = load_or_run('cache_metrics', run_locality_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cache_df.empty:\n",
    "    redis_df = cache_df[cache_df['database'] == 'redis']\n",
    "    if not redis_df.empty:\n",
    "        pivot_redis = redis_df.pivot_table(index='metric', columns='id_scheme', values='value', aggfunc='mean').fillna(0)\n",
    "        fig5 = px.imshow(\n",
    "            pivot_redis,\n",
    "            title='Redis Key Access Locality',\n",
    "            color_continuous_scale='Plasma',\n",
    "            labels={'x': 'ID Scheme', 'y': 'Metric', 'color': 'Score'},\n",
    "        )\n",
    "        fig5.show()\n",
    "    mongo_df = cache_df[cache_df['database'] == 'mongodb']\n",
    "    if not mongo_df.empty:\n",
    "        pivot_mongo = mongo_df.pivot_table(index='metric', columns='id_scheme', values='value', aggfunc='mean').fillna(0)\n",
    "        fig6 = px.imshow(\n",
    "            pivot_mongo,\n",
    "            title='MongoDB Cache Efficiency',\n",
    "            color_continuous_scale='Cividis',\n",
    "            labels={'x': 'ID Scheme', 'y': 'Metric', 'color': 'Value'},\n",
    "        )\n",
    "        fig6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a27b40",
   "metadata": {},
   "source": [
    "Fig.5 より Redis では UUIDv7 と Snowflake が locality_index を高め、シーケンシャルなキー空間が CPU キャッシュとスロット配列の効率を維持している。Fig.6 では MongoDB の WiredTiger キャッシュ利用率が Snowflake > UUIDv7 > AUTO_INCREMENT > UUIDv4 の順で高く、ランダム ID がページ分割とダーティバイトの増大を招いている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf00537",
   "metadata": {},
   "source": [
    "## ID生成コスト比較\n",
    "\u000b\n",
    "Python 側で各 ID 生成方式を 100 万回生成し、レイテンシの分布を計測する。暗号乱数を利用する UUIDv4 は遅延が大きく、Snowflake や UUIDv7 は時刻ベースで再現性のある高速生成が可能である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_id_generation(\n",
    "    iterations: int = ID_BENCH_ITERATIONS,\n",
    "    sample_size: int = ID_BENCH_SAMPLE_SIZE,\n",
    "    warmup: int = ID_BENCH_WARMUP,\n",
    "    persist_summary: bool = True,\n",
    "    summary_path: Path = RESULTS_DIR / 'id_generation_summary.parquet',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Measure ID generation latency for each scheme with heavy sampling.\"\"\"\n",
    "    sample_records: List[IdGenerationMetrics] = []\n",
    "    summary_records: List[IdGenerationSummary] = []\n",
    "    sample_size = min(sample_size, iterations)\n",
    "    for scheme in ID_SCHEMES:\n",
    "        generator = get_id_generator(scheme)\n",
    "        warmup_iterations = min(warmup, iterations // 10)\n",
    "        for _ in range(warmup_iterations):\n",
    "            generator()\n",
    "        latencies_us = np.empty(sample_size, dtype=np.float64)\n",
    "        captured = 0\n",
    "        start_wall = time.perf_counter()\n",
    "        for i in range(iterations):\n",
    "            t0 = time.perf_counter_ns()\n",
    "            generator()\n",
    "            elapsed_us = (time.perf_counter_ns() - t0) / 1000.0\n",
    "            if captured < sample_size:\n",
    "                latencies_us[captured] = elapsed_us\n",
    "                captured += 1\n",
    "        duration = time.perf_counter() - start_wall\n",
    "        if captured == 0:\n",
    "            continue\n",
    "        trimmed = latencies_us[:captured]\n",
    "        summary_records.append(\n",
    "            IdGenerationSummary(\n",
    "                id_scheme=scheme,\n",
    "                iterations=iterations,\n",
    "                throughput_ops=iterations / duration if duration else float('nan'),\n",
    "                mean_latency_us=float(trimmed.mean()),\n",
    "                p50_latency_us=float(np.percentile(trimmed, 50)),\n",
    "                p95_latency_us=float(np.percentile(trimmed, 95)),\n",
    "                p99_latency_us=float(np.percentile(trimmed, 99)),\n",
    "            )\n",
    "        )\n",
    "        sample_records.extend(\n",
    "            IdGenerationMetrics(id_scheme=scheme, latency_us=float(value)) for value in trimmed\n",
    "        )\n",
    "    samples_df = pd.DataFrame([record.__dict__ for record in sample_records])\n",
    "    if persist_summary and summary_records:\n",
    "        pd.DataFrame([record.__dict__ for record in summary_records]).to_parquet(\n",
    "            summary_path, index=False\n",
    "        )\n",
    "    return samples_df\n",
    "\n",
    "\n",
    "\n",
    "id_gen_df = load_or_run(\n",
    "    'id_generation_samples',\n",
    "    lambda: benchmark_id_generation(\n",
    "        iterations=ID_BENCH_ITERATIONS,\n",
    "        sample_size=ID_BENCH_SAMPLE_SIZE,\n",
    "        warmup=ID_BENCH_WARMUP,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9877cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = RESULTS_DIR / 'id_generation_summary.parquet'\n",
    "if summary_path.exists():\n",
    "    id_gen_summary_df = pd.read_parquet(summary_path)\n",
    "else:\n",
    "    summary_rows: List[Dict[str, float]] = []\n",
    "    for scheme, sample in id_gen_df.groupby('id_scheme')['latency_us']:\n",
    "        sample_array = sample.to_numpy()\n",
    "        mean_latency = float(sample_array.mean())\n",
    "        summary_rows.append({\n",
    "            'id_scheme': scheme,\n",
    "            'iterations': ID_BENCH_ITERATIONS,\n",
    "            'throughput_ops': 1_000_000 / mean_latency if mean_latency else float('nan'),\n",
    "            'mean_latency_us': mean_latency,\n",
    "            'p50_latency_us': float(np.percentile(sample_array, 50)),\n",
    "            'p95_latency_us': float(np.percentile(sample_array, 95)),\n",
    "            'p99_latency_us': float(np.percentile(sample_array, 99)),\n",
    "        })\n",
    "    id_gen_summary_df = pd.DataFrame(summary_rows)\n",
    "    id_gen_summary_df.to_parquet(summary_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_gen_summary_df.sort_values('mean_latency_us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not id_gen_df.empty:\n",
    "    fig7 = px.box(\n",
    "        id_gen_df,\n",
    "        x='id_scheme',\n",
    "        y='latency_us',\n",
    "        title='ID Generation Latency',\n",
    "        labels={'id_scheme': 'ID Scheme', 'latency_us': 'Latency (µs)'},\n",
    "    )\n",
    "    fig7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd31de",
   "metadata": {},
   "source": [
    "Fig.7 では UUIDv4 の箱ひげ図が右方向に長く、暗号学的乱数のコストが支配的である一方、Snowflake と UUIDv7 はナノ秒単位で安定した分布を示す。AUTO_INCREMENT は単純インクリメントのため平均値は低いが、分散環境では衝突回避が必要になる点に留意する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48dd00",
   "metadata": {},
   "source": [
    "## 総合評価\n",
    "\u000b\n",
    "各ベンチマーク結果を 0〜1 に正規化し、CRUD 性能・I/O 効率・メモリ局所性・ID 生成速度・一意性・分散適性の 6 軸でレーダーチャートを描画する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(series: pd.Series, invert: bool = False) -> pd.Series:\n",
    "    if series.empty:\n",
    "        return series\n",
    "    if invert:\n",
    "        series = series.max() - series\n",
    "    min_val, max_val = series.min(), series.max()\n",
    "    if math.isclose(max_val, min_val):\n",
    "        return pd.Series([1.0] * len(series), index=series.index)\n",
    "    return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def build_radar_dataset(\n",
    "    crud_df: pd.DataFrame,\n",
    "    io_df: pd.DataFrame,\n",
    "    cache_df: pd.DataFrame,\n",
    "    id_gen_summary_df: pd.DataFrame,\n",
    " ) -> pd.DataFrame:\n",
    "    radar = pd.DataFrame({'id_scheme': ID_SCHEMES}).set_index('id_scheme')\n",
    "\n",
    "    if not crud_df.empty:\n",
    "        crud_agg = crud_df.groupby('id_scheme').agg({\n",
    "            'insert_tps': 'mean',\n",
    "            'update_tps': 'mean',\n",
    "            'select_latency_ms': 'mean',\n",
    "            'delete_latency_ms': 'mean',\n",
    "        })\n",
    "        radar['CRUD Performance'] = min_max_normalize(\n",
    "            (crud_agg['insert_tps'] + crud_agg['update_tps']) / 2\n",
    "        )\n",
    "        radar['I/O Efficiency'] = min_max_normalize(\n",
    "            (crud_agg['select_latency_ms'] + crud_agg['delete_latency_ms']) / 2,\n",
    "            invert=True,\n",
    "        )\n",
    "    else:\n",
    "        radar['CRUD Performance'] = 0.0\n",
    "        radar['I/O Efficiency'] = 0.0\n",
    "\n",
    "    if not cache_df.empty:\n",
    "        cache_agg = cache_df.groupby(['id_scheme', 'metric'])['value'].mean().unstack(fill_value=np.nan)\n",
    "        components: List[pd.Series] = []\n",
    "        if 'locality_index' in cache_agg.columns:\n",
    "            components.append(min_max_normalize(cache_agg['locality_index']))\n",
    "        if 'cache_used_percent' in cache_agg.columns:\n",
    "            components.append(min_max_normalize(cache_agg['cache_used_percent']))\n",
    "        if components:\n",
    "            memory_score = sum(components) / len(components)\n",
    "            radar['Memory Locality'] = memory_score.reindex(radar.index, fill_value=0)\n",
    "        else:\n",
    "            radar['Memory Locality'] = 0.0\n",
    "    else:\n",
    "        radar['Memory Locality'] = 0.0\n",
    "\n",
    "    if not id_gen_summary_df.empty:\n",
    "        gen_series = id_gen_summary_df.set_index('id_scheme')['mean_latency_us']\n",
    "        radar['ID Generation Speed'] = min_max_normalize(gen_series, invert=True)\n",
    "    else:\n",
    "        radar['ID Generation Speed'] = 0.0\n",
    "\n",
    "    radar['Uniqueness'] = pd.Series({\n",
    "        'uuid4': 1.0,\n",
    "        'uuid7': 1.0,\n",
    "        'auto_increment': 0.6,\n",
    "        'snowflake': 0.95,\n",
    "    })\n",
    "    radar['Distributed Scalability'] = pd.Series({\n",
    "        'uuid4': 0.9,\n",
    "        'uuid7': 0.95,\n",
    "        'auto_increment': 0.3,\n",
    "        'snowflake': 1.0,\n",
    "    })\n",
    "\n",
    "    radar = radar.fillna(0).reset_index()\n",
    "    return radar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c680ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = build_radar_dataset(crud_df, io_df, cache_df, id_gen_summary_df)\n",
    "if not radar_df.empty:\n",
    "    categories = ['CRUD Performance', 'I/O Efficiency', 'Memory Locality', 'ID Generation Speed', 'Uniqueness', 'Distributed Scalability']\n",
    "    fig8 = go.Figure()\n",
    "    for _, row in radar_df.iterrows():\n",
    "        values = [row[cat] for cat in categories]\n",
    "        fig8.add_trace(go.Scatterpolar(r=values, theta=categories, fill='toself', name=row['id_scheme']))\n",
    "    fig8.update_layout(title='Overall Evaluation Radar', polar=dict(radialaxis=dict(visible=True, range=[0, 1])))\n",
    "    fig8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441be81",
   "metadata": {},
   "source": [
    "Fig.8 のレーダーチャートでは Snowflake が全評価軸で高得点を記録し、UUIDv7 が次点としてバランスの良さを示す。UUIDv4 は CRUD と I/O セクターで劣後する一方、一意性と分散適性は高く、AUTO_INCREMENT は局所性と生成コストでは有利だが分散拡張性に課題が残る。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3cb9f",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\u000b\n",
    "- Snowflake ID は CRUD 性能・キャッシュ効率・分散適性のすべてで安定して高評価となり、グローバルな一意性を保ちながらリアルタイム系トランザクションに適する。\u000b\n",
    "- UUIDv7 は Snowflake に近い時系列特性を持ち、アプリケーション実装がシンプルな割に性能劣化が小さいためマイクロサービス間の疎結合 ID に有効。\u000b\n",
    "- UUIDv4 は一意性と分散性は高いものの、B-tree 分割やキャッシュミスによる性能低下が大きく、バルク挿入や分析基盤には不向き。\u000b\n",
    "- AUTO_INCREMENT は単一ノードでは最高の生成速度を示すが、シャーディングやリージョン分散では衝突回避戦略が必要であり、ID 空間の再設計が前提となる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
