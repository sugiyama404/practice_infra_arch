{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5524283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports are ready in one place.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import socket\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import sqlalchemy\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print('All imports are ready in one place.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e25e9",
   "metadata": {},
   "source": [
    "# Step 1: Pre-check and Environment Setup\n",
    "\n",
    "This cell checks for Docker, determines the correct `docker-compose` command, and verifies that no conflicting containers are running. This is a safety check to prevent errors when starting the services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3fc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for service connectivity\n",
    "def is_port_open(host: str, port: int, timeout: float = 1.0) -> bool:\n",
    "    \"\"\"Check if a TCP port is open on the given host.\"\"\"\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=timeout):\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075d7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check endpoints\n",
    "HEALTH_ENDPOINTS = [\n",
    "    (\"choreography\", \"order-service\", 'http://localhost:8011/health'),\n",
    "    (\"orchestration\", \"saga-orchestrator\", 'http://localhost:8005/health'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a36d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service endpoints for performance testing\n",
    "ENDPOINTS = {\n",
    "    'choreography': 'http://localhost:8001/orders',\n",
    "    'orchestration': 'http://localhost:8011/orders'\n",
    "}\n",
    "\n",
    "# Health check endpoints (streamlined)\n",
    "HEALTH_ENDPOINTS = [\n",
    "    (\"choreography\", \"order-service\", 'http://localhost:8001/health'),\n",
    "    (\"orchestration\", \"saga-orchestrator\", 'http://localhost:8005/health'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198a048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Service Health Check ===\n",
      "choreography | order-service   | ✗ Cannot connect to host localho...\n",
      "orchestration | saga-orchestrator | ✓\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick health check for services\n",
    "print(\"=== Service Health Check ===\")\n",
    "async def quick_health_check():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for pattern, service_name, health_url in HEALTH_ENDPOINTS:\n",
    "            try:\n",
    "                async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=5)) as response:\n",
    "                    status = \"✓\" if response.status == 200 else f\"✗ {response.status}\"\n",
    "                    print(f\"{pattern:12} | {service_name:15} | {status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{pattern:12} | {service_name:15} | ✗ {str(e)[:30]}...\")\n",
    "\n",
    "await quick_health_check()\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977ec13",
   "metadata": {},
   "source": [
    "# Quick Service Health Check\n",
    "\n",
    "This cell performs a quick health check of the required services before running performance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4079c65",
   "metadata": {},
   "source": [
    "# Performance Testing and CSV Export for Saga Pattern\n",
    "\n",
    "## データリセットが必要な場合\n",
    "\n",
    "テストデータが古い、または不整合がある場合は以下を実行してください：\n",
    "\n",
    "```bash\n",
    "cd saga_pattern/choreography_pattern\n",
    "docker-compose down\n",
    "docker-compose build --no-cache\n",
    "docker-compose up -d\n",
    "\n",
    "cd ../orchestration_pattern  \n",
    "docker-compose down\n",
    "docker-compose build --no-cache\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "## テスト概要\n",
    "\n",
    "以下の3段階のテストを実行し、Sagaパターンの性能と整合性を検証します：\n",
    "\n",
    "1. **単発テスト**: Choreography/Orchestration 各5回（機能確認）\n",
    "2. **異常ケーステスト**: 在庫不足・決済失敗を各10回（補償確認）  \n",
    "3. **負荷テスト**: 100 VU × 3分間で約15,000注文（分位数計算用）\n",
    "\n",
    "## 出力CSV\n",
    "\n",
    "- `e2e_latency.csv`: E2Eレスポンス時間（p50/p95/p99算出用）\n",
    "- `convergence_events.csv`: イベント収束時間（ヒストグラム用）\n",
    "- `saga_steps.csv`: サガステップ詳細（補償タイムライン用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3434610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance testing setup complete.\n",
      "Database connection: mysql+pymysql://cloudmart_user:cloudmart_pass@localhost:3307/cloudmart_saga\n",
      "Endpoints: {'choreography': 'http://localhost:8011/orders', 'orchestration': 'http://localhost:8005/saga/start'}\n",
      "Utility functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Database connection settings\n",
    "DB_CONFIG = {\n",
    "    'user': 'cloudmart_user',\n",
    "    'password': 'cloudmart_pass',\n",
    "    'host': 'localhost',\n",
    "    'port': 3307,  # adjusted to match docker-compose host mapping (host 3307 -> container 3306)\n",
    "    'database': 'cloudmart_saga'\n",
    "}\n",
    "\n",
    "CONN_STR = f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "# Test endpoints\n",
    "ENDPOINTS = {\n",
    "    'choreography': 'http://localhost:8011/orders',\n",
    "    'orchestration': 'http://localhost:8005/saga/start'\n",
    "}\n",
    "\n",
    "# Test results storage\n",
    "test_results = []\n",
    "\n",
    "print(\"Performance testing setup complete.\")\n",
    "print(f\"Database connection: {CONN_STR}\")\n",
    "print(f\"Endpoints: {ENDPOINTS}\")\n",
    "\n",
    "# Utility function to generate test data with failure injection\n",
    "def generate_test_payload(scenario='success', pattern='choreography'):\n",
    "    \"\"\"Generate test payload with failure injection logic\"\"\"\n",
    "    # Use valid customer and book IDs that exist in the database\n",
    "    valid_customers = [\"customer-001\", \"customer-002\", \"customer-003\", \"customer-004\", \"customer-005\"]\n",
    "    valid_books = [\"book-123\", \"book-456\", \"book-789\", \"book-101\", \"book-202\"]\n",
    "\n",
    "    base_customer = random.choice(valid_customers)\n",
    "    base_book = random.choice(valid_books)\n",
    "\n",
    "    # Failure injection logic\n",
    "    if scenario == 'stock_failure':\n",
    "        # Use very high quantity to trigger stock failure (inventory won't have enough)\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 9999}]\n",
    "        }\n",
    "    elif scenario == 'payment_failure':\n",
    "        # Use high-value book to trigger payment failure (amount > 5000)\n",
    "        # The book price in the system is around 3500, so quantity 2 should exceed 5000\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 2}]\n",
    "        }\n",
    "    else:\n",
    "        # Normal success case\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 1}]\n",
    "        }\n",
    "# Async HTTP client utilities\n",
    "async def make_request(session, url, payload, pattern, scenario):\n",
    "    \"\"\"Make async HTTP request and record timing\"\"\"\n",
    "    request_id = uuid.uuid4().hex[:8]\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "            response_time = time.time() - start_time\n",
    "\n",
    "            if response.status == 200 or response.status == 201:\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    order_id = result.get('order_id', result.get('id', request_id))\n",
    "                except Exception:\n",
    "                    order_id = request_id\n",
    "                    result = await response.text()\n",
    "            else:\n",
    "                order_id = request_id\n",
    "                result = await response.text()\n",
    "\n",
    "            return {\n",
    "                'saga_pattern': pattern,\n",
    "                'scenario': scenario,\n",
    "                'order_id': order_id,\n",
    "                'request_id': request_id,\n",
    "                'status_code': response.status,\n",
    "                'response_time': response_time,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'result': str(result)[:200],  # Truncate long results\",\n",
    "                'load_phase': 'single'  # Default for single tests, will be updated for load tests\",\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        response_time = time.time() - start_time\n",
    "        return {\n",
    "            'saga_pattern': pattern,\n",
    "            'scenario': scenario,\n",
    "            'order_id': request_id,\n",
    "            'request_id': request_id,\n",
    "            'status_code': 'ERROR',\n",
    "            'response_time': response_time,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'result': str(e)[:200],\n",
    "            'load_phase': 'single'  # Default for single tests, will be updated for load tests\",\n",
    "        }\n",
    "\n",
    "print(\"Utility functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1010c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous test results cleared. Ready for new tests.\n"
     ]
    }
   ],
   "source": [
    "# Clear previous test results to start fresh\n",
    "test_results = []\n",
    "print(\"Previous test results cleared. Ready for new tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "162a1711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Updated Failure Injection ===\n",
      "\n",
      "1. Testing success case:\n",
      "   Payload: {'customer_id': 'customer-001', 'items': [{'book_id': 'book-123', 'quantity': 1}]}\n",
      "   Result: 200 - 0.059s\n",
      "\n",
      "2. Testing stock failure (high quantity):\n",
      "   Payload: {'customer_id': 'customer-003', 'items': [{'book_id': 'book-101', 'quantity': 9999}]}\n",
      "   Result: 200 - 0.020s\n",
      "\n",
      "3. Testing payment failure (high amount):\n",
      "   Payload: {'customer_id': 'customer-003', 'items': [{'book_id': 'book-202', 'quantity': 2}]}\n",
      "   Result: 200 - 0.011s\n"
     ]
    }
   ],
   "source": [
    "# Test the updated failure injection logic with a few manual tests\n",
    "async def test_failure_injection():\n",
    "    \"\"\"Quick test of the updated failure injection logic\"\"\"\n",
    "    print(\"=== Testing Updated Failure Injection ===\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Test 1: Success case\n",
    "        print(\"\\n1. Testing success case:\")\n",
    "        payload = generate_test_payload('success')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['choreography'], payload, 'choreography', 'success')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "        # Test 2: Stock failure case\n",
    "        print(\"\\n2. Testing stock failure (high quantity):\")\n",
    "        payload = generate_test_payload('stock_failure')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['choreography'], payload, 'choreography', 'stock_failure')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "        # Test 3: Payment failure case\n",
    "        print(\"\\n3. Testing payment failure (high amount):\")\n",
    "        payload = generate_test_payload('payment_failure')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['choreography'], payload, 'choreography', 'payment_failure')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "await test_failure_injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1865c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Single-shot Tests ===\n",
      "\n",
      "Testing choreography - success (5 times)\n",
      "   1. ✓ 200 - 0.028s - order-bceddc45\n",
      "   2. ✓ 200 - 0.022s - order-bc903ba9\n",
      "   3. ✓ 200 - 0.023s - order-887d0c5a\n",
      "   4. ✓ 200 - 0.030s - order-f5f45cb7\n",
      "   3. ✓ 200 - 0.023s - order-887d0c5a\n",
      "   4. ✓ 200 - 0.030s - order-f5f45cb7\n",
      "   5. ✓ 200 - 0.030s - order-3685466d\n",
      "\n",
      "Testing orchestration - success (5 times)\n",
      "   1. ✓ 200 - 0.049s - order-746e17e6\n",
      "   5. ✓ 200 - 0.030s - order-3685466d\n",
      "\n",
      "Testing orchestration - success (5 times)\n",
      "   1. ✓ 200 - 0.049s - order-746e17e6\n",
      "   2. ✓ 200 - 0.026s - order-6ca9fc2b\n",
      "   3. ✓ 200 - 0.025s - order-1a3a9281\n",
      "   2. ✓ 200 - 0.026s - order-6ca9fc2b\n",
      "   3. ✓ 200 - 0.025s - order-1a3a9281\n",
      "   4. ✓ 200 - 0.033s - order-9de98541\n",
      "   5. ✓ 200 - 0.028s - order-32e49f29\n",
      "   4. ✓ 200 - 0.033s - order-9de98541\n",
      "   5. ✓ 200 - 0.028s - order-32e49f29\n",
      "\n",
      "Testing choreography - stock_failure (10 times)\n",
      "   1. ✓ 200 - 0.015s - order-94a48bb2\n",
      "   2. ✓ 200 - 0.033s - order-ec1da7f7\n",
      "\n",
      "Testing choreography - stock_failure (10 times)\n",
      "   1. ✓ 200 - 0.015s - order-94a48bb2\n",
      "   2. ✓ 200 - 0.033s - order-ec1da7f7\n",
      "   3. ✓ 200 - 0.030s - order-43623286\n",
      "   4. ✓ 200 - 0.039s - order-7e9d87c3\n",
      "   3. ✓ 200 - 0.030s - order-43623286\n",
      "   4. ✓ 200 - 0.039s - order-7e9d87c3\n",
      "   5. ✓ 200 - 0.108s - order-434d4f83\n",
      "   6. ✓ 200 - 0.013s - order-bf44caa2\n",
      "   5. ✓ 200 - 0.108s - order-434d4f83\n",
      "   6. ✓ 200 - 0.013s - order-bf44caa2\n",
      "   7. ✓ 200 - 0.015s - order-3b933d1b\n",
      "   8. ✓ 200 - 0.018s - order-273f9ea0\n",
      "   7. ✓ 200 - 0.015s - order-3b933d1b\n",
      "   8. ✓ 200 - 0.018s - order-273f9ea0\n",
      "   9. ✓ 200 - 0.029s - order-0f205031\n",
      "  10. ✓ 200 - 0.029s - order-a1242df5\n",
      "   9. ✓ 200 - 0.029s - order-0f205031\n",
      "  10. ✓ 200 - 0.029s - order-a1242df5\n",
      "\n",
      "Testing choreography - payment_failure (10 times)\n",
      "   1. ✓ 200 - 0.031s - order-45d85b47\n",
      "   2. ✓ 200 - 0.029s - order-8a13bda4\n",
      "\n",
      "Testing choreography - payment_failure (10 times)\n",
      "   1. ✓ 200 - 0.031s - order-45d85b47\n",
      "   2. ✓ 200 - 0.029s - order-8a13bda4\n",
      "   3. ✓ 200 - 0.057s - order-54583bea\n",
      "   4. ✓ 200 - 0.031s - order-9690c749\n",
      "   3. ✓ 200 - 0.057s - order-54583bea\n",
      "   4. ✓ 200 - 0.031s - order-9690c749\n",
      "   5. ✓ 200 - 0.018s - order-deda5711\n",
      "   6. ✓ 200 - 0.030s - order-0c6feebb\n",
      "   5. ✓ 200 - 0.018s - order-deda5711\n",
      "   6. ✓ 200 - 0.030s - order-0c6feebb\n",
      "   7. ✓ 200 - 0.031s - order-70de8a95\n",
      "   8. ✓ 200 - 0.033s - order-8d65f607\n",
      "   7. ✓ 200 - 0.031s - order-70de8a95\n",
      "   8. ✓ 200 - 0.033s - order-8d65f607\n",
      "   9. ✓ 200 - 0.030s - order-6fb7fef3\n",
      "  10. ✓ 200 - 0.032s - order-1a6b319c\n",
      "   9. ✓ 200 - 0.030s - order-6fb7fef3\n",
      "  10. ✓ 200 - 0.032s - order-1a6b319c\n",
      "\n",
      "Testing orchestration - stock_failure (10 times)\n",
      "   1. ✓ 200 - 0.040s - order-1b9d3fdf\n",
      "   2. ✓ 200 - 0.026s - order-3c0d1227\n",
      "\n",
      "Testing orchestration - stock_failure (10 times)\n",
      "   1. ✓ 200 - 0.040s - order-1b9d3fdf\n",
      "   2. ✓ 200 - 0.026s - order-3c0d1227\n",
      "   3. ✓ 200 - 0.026s - order-021e9689\n",
      "   4. ✓ 200 - 0.023s - order-210e331e\n",
      "   3. ✓ 200 - 0.026s - order-021e9689\n",
      "   4. ✓ 200 - 0.023s - order-210e331e\n",
      "   5. ✓ 200 - 0.024s - order-aef33253\n",
      "   6. ✓ 200 - 0.027s - order-1dc3c1b6\n",
      "   5. ✓ 200 - 0.024s - order-aef33253\n",
      "   6. ✓ 200 - 0.027s - order-1dc3c1b6\n",
      "   7. ✓ 200 - 0.029s - order-6c8a4138\n",
      "   8. ✓ 200 - 0.029s - order-ae6fe682\n",
      "   7. ✓ 200 - 0.029s - order-6c8a4138\n",
      "   8. ✓ 200 - 0.029s - order-ae6fe682\n",
      "   9. ✓ 200 - 0.022s - order-b33d15cb\n",
      "  10. ✓ 200 - 0.017s - order-5ff15d6c\n",
      "   9. ✓ 200 - 0.022s - order-b33d15cb\n",
      "  10. ✓ 200 - 0.017s - order-5ff15d6c\n",
      "\n",
      "Testing orchestration - payment_failure (10 times)\n",
      "   1. ✓ 200 - 0.021s - order-96188587\n",
      "   2. ✓ 200 - 0.022s - order-f46eef25\n",
      "\n",
      "Testing orchestration - payment_failure (10 times)\n",
      "   1. ✓ 200 - 0.021s - order-96188587\n",
      "   2. ✓ 200 - 0.022s - order-f46eef25\n",
      "   3. ✓ 200 - 0.021s - order-958d359c\n",
      "   4. ✓ 200 - 0.029s - order-4bb6e383\n",
      "   3. ✓ 200 - 0.021s - order-958d359c\n",
      "   4. ✓ 200 - 0.029s - order-4bb6e383\n",
      "   5. ✓ 200 - 0.026s - order-5ed2b494\n",
      "   6. ✓ 200 - 0.028s - order-7fff1ad4\n",
      "   5. ✓ 200 - 0.026s - order-5ed2b494\n",
      "   6. ✓ 200 - 0.028s - order-7fff1ad4\n",
      "   7. ✓ 200 - 0.033s - order-86630781\n",
      "   8. ✓ 200 - 0.016s - order-36fba501\n",
      "   7. ✓ 200 - 0.033s - order-86630781\n",
      "   8. ✓ 200 - 0.016s - order-36fba501\n",
      "   9. ✓ 200 - 0.016s - order-c2873940\n",
      "  10. ✓ 200 - 0.020s - order-b8bbf2bf\n",
      "   9. ✓ 200 - 0.016s - order-c2873940\n",
      "  10. ✓ 200 - 0.020s - order-b8bbf2bf\n",
      "\n",
      "Single tests completed. Total results: 50\n",
      "\n",
      "Single Test Summary:\n",
      "                              response_time               status_code\n",
      "                                      count   mean    std    <lambda>\n",
      "pattern       scenario                                               \n",
      "choreography  payment_failure            10  0.032  0.010          10\n",
      "              stock_failure              10  0.033  0.028          10\n",
      "              success                     5  0.027  0.004           5\n",
      "orchestration payment_failure            10  0.023  0.006          10\n",
      "              stock_failure              10  0.026  0.006          10\n",
      "              success                     5  0.032  0.010           5\n",
      "\n",
      "Single tests completed. Total results: 50\n",
      "\n",
      "Single Test Summary:\n",
      "                              response_time               status_code\n",
      "                                      count   mean    std    <lambda>\n",
      "pattern       scenario                                               \n",
      "choreography  payment_failure            10  0.032  0.010          10\n",
      "              stock_failure              10  0.033  0.028          10\n",
      "              success                     5  0.027  0.004           5\n",
      "orchestration payment_failure            10  0.023  0.006          10\n",
      "              stock_failure              10  0.026  0.006          10\n",
      "              success                     5  0.032  0.010           5\n"
     ]
    }
   ],
   "source": [
    "# Single-shot and abnormal case tests\n",
    "async def run_single_tests():\n",
    "    \"\"\"Run single-shot tests for functional verification\"\"\"\n",
    "    print(\"=== Running Single-shot Tests ===\")\n",
    "\n",
    "    test_cases = [\n",
    "        # Normal success cases (5 times each)\n",
    "        ('choreography', 'success', 5),\n",
    "        ('orchestration', 'success', 5),\n",
    "\n",
    "        # Failure cases (10 times each)\n",
    "        ('choreography', 'stock_failure', 10),\n",
    "        ('choreography', 'payment_failure', 10),\n",
    "        ('orchestration', 'stock_failure', 10),\n",
    "        ('orchestration', 'payment_failure', 10),\n",
    "    ]\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for pattern, scenario, count in test_cases:\n",
    "            print(f\"\\nTesting {pattern} - {scenario} ({count} times)\")\n",
    "            url = ENDPOINTS[pattern]\n",
    "\n",
    "            for i in range(count):\n",
    "                payload = generate_test_payload(scenario, pattern)\n",
    "                result = await make_request(session, url, payload, pattern, scenario)\n",
    "                test_results.append(result)\n",
    "\n",
    "                status_symbol = \"✓\" if result['status_code'] in [200, 201] else \"✗\"\n",
    "                print(f\"  {i+1:2d}. {status_symbol} {result['status_code']} - {result['response_time']:.3f}s - {result['order_id']}\")\n",
    "\n",
    "                # Brief delay between requests\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "# Run single tests\n",
    "await run_single_tests()\n",
    "\n",
    "print(f\"\\nSingle tests completed. Total results: {len(test_results)}\")\n",
    "\n",
    "# Show summary\n",
    "df_single = pd.DataFrame(test_results)\n",
    "if not df_single.empty:\n",
    "    summary = df_single.groupby(['pattern', 'scenario']).agg({\n",
    "        'response_time': ['count', 'mean', 'std'],\n",
    "        'status_code': lambda x: (x.isin([200, 201])).sum()\n",
    "    }).round(3)\n",
    "    print(\"\\nSingle Test Summary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e2558eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Results Summary ===\n",
      "Total test results: 50\n",
      "Successful requests: 50\n",
      "Error requests: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "Status code distribution:\n",
      "status_code\n",
      "200    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check test results summary\n",
    "print(\"=== Test Results Summary ===\")\n",
    "print(f\"Total test results: {len(test_results)}\")\n",
    "\n",
    "if test_results:\n",
    "    df = pd.DataFrame(test_results)\n",
    "    success_count = (df['status_code'].isin([200, 201])).sum()\n",
    "    error_count = (df['status_code'] == 'ERROR').sum()\n",
    "    print(f\"Successful requests: {success_count}\")\n",
    "    print(f\"Error requests: {error_count}\")\n",
    "    print(f\"Success rate: {success_count / len(test_results) * 100:.1f}%\")\n",
    "\n",
    "    if error_count > 0:\n",
    "        print(\"\\nSample error details:\")\n",
    "        error_results = df[df['status_code'] == 'ERROR'].head(3)\n",
    "        for _, row in error_results.iterrows():\n",
    "            print(f\"  {row['pattern']} - {row['scenario']}: {row['result'][:100]}...\")\n",
    "\n",
    "    print(\"\\nStatus code distribution:\")\n",
    "    print(df['status_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479cfc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Multi-Phase Load Tests ===\n",
      "This will take approximately 6 minutes total (3 phases).\n",
      "\n",
      "--- Phase 1: Light Load (Warm-up) ---\n",
      "Duration: 90s, Virtual Users: 30\n",
      "=== Running Load Test ===\n",
      "Duration: 90s, Virtual Users: 30\n",
      "Expected requests: ~1350 (assuming 0.5 req/s per VU)\n",
      "Worker  11 completed 46 requests\n",
      "Worker   5 completed 45 requests\n",
      "Worker  11 completed 46 requests\n",
      "Worker   5 completed 45 requests\n",
      "Worker  29 completed 44 requests\n",
      "Worker  21 completed 45 requests\n",
      "Worker  16 completed 46 requests\n",
      "Worker  29 completed 44 requests\n",
      "Worker  21 completed 45 requests\n",
      "Worker  16 completed 46 requests\n",
      "Worker   7 completed 45 requests\n",
      "Worker   4 completed 45 requests\n",
      "Worker   9 completed 44 requests\n",
      "Worker  15 completed 46 requests\n",
      "Worker  17 completed 44 requests\n",
      "Worker  27 completed 45 requests\n",
      "Worker   7 completed 45 requests\n",
      "Worker   4 completed 45 requests\n",
      "Worker   9 completed 44 requests\n",
      "Worker  15 completed 46 requests\n",
      "Worker  17 completed 44 requests\n",
      "Worker  27 completed 45 requests\n",
      "Worker   8 completed 44 requests\n",
      "Worker   6 completed 44 requests\n",
      "Worker  14 completed 45 requests\n",
      "Worker  18 completed 47 requests\n",
      "Worker  28 completed 45 requests\n",
      "Worker   8 completed 44 requests\n",
      "Worker   6 completed 44 requests\n",
      "Worker  14 completed 45 requests\n",
      "Worker  18 completed 47 requests\n",
      "Worker  28 completed 45 requests\n",
      "Worker  25 completed 45 requests\n",
      "Worker  23 completed 45 requests\n",
      "Worker  25 completed 45 requests\n",
      "Worker  23 completed 45 requests\n",
      "Worker  20 completed 45 requests\n",
      "Worker   2 completed 44 requests\n",
      "Worker  12 completed 44 requests\n",
      "Worker   1 completed 45 requests\n",
      "Worker  26 completed 46 requests\n",
      "Worker  20 completed 45 requests\n",
      "Worker   2 completed 44 requests\n",
      "Worker  12 completed 44 requests\n",
      "Worker   1 completed 45 requests\n",
      "Worker  26 completed 46 requests\n",
      "Worker   3 completed 45 requests\n",
      "Worker  22 completed 45 requests\n",
      "Worker  10 completed 45 requests\n",
      "Worker  24 completed 46 requests\n",
      "Worker   3 completed 45 requests\n",
      "Worker  22 completed 45 requests\n",
      "Worker  10 completed 45 requests\n",
      "Worker  24 completed 46 requests\n",
      "Worker  13 completed 45 requests\n",
      "Worker  19 completed 45 requests\n",
      "Worker  13 completed 45 requests\n",
      "Worker  19 completed 45 requests\n",
      "Worker   0 completed 46 requests\n",
      "\n",
      "Load test completed in 91.9s\n",
      "Total requests generated: 1351\n",
      "Load Test Summary:\n",
      "Success rate: 100.0%\n",
      "Average response time: 0.024s\n",
      "P95 response time: 0.048s\n",
      "Scenario distribution:\n",
      "  load_success: 1220 (90.3%)\n",
      "  load_stock_failure: 84 (6.2%)\n",
      "  load_payment_failure: 47 (3.5%)\n",
      "Pausing 10 seconds between phases...\n",
      "Worker   0 completed 46 requests\n",
      "\n",
      "Load test completed in 91.9s\n",
      "Total requests generated: 1351\n",
      "Load Test Summary:\n",
      "Success rate: 100.0%\n",
      "Average response time: 0.024s\n",
      "P95 response time: 0.048s\n",
      "Scenario distribution:\n",
      "  load_success: 1220 (90.3%)\n",
      "  load_stock_failure: 84 (6.2%)\n",
      "  load_payment_failure: 47 (3.5%)\n",
      "Pausing 10 seconds between phases...\n",
      "\n",
      "--- Phase 2: Medium Load (Standard) ---\n",
      "Duration: 120s, Virtual Users: 80\n",
      "=== Running Load Test ===\n",
      "Duration: 120s, Virtual Users: 80\n",
      "Expected requests: ~4800 (assuming 0.5 req/s per VU)\n",
      "\n",
      "--- Phase 2: Medium Load (Standard) ---\n",
      "Duration: 120s, Virtual Users: 80\n",
      "=== Running Load Test ===\n",
      "Duration: 120s, Virtual Users: 80\n",
      "Expected requests: ~4800 (assuming 0.5 req/s per VU)\n",
      "Worker  33 completed 59 requests\n",
      "Worker  14 completed 60 requests\n",
      "Worker  57 completed 59 requests\n",
      "Worker  23 completed 58 requests\n",
      "Worker  20 completed 60 requests\n",
      "Worker  33 completed 59 requests\n",
      "Worker  14 completed 60 requests\n",
      "Worker  57 completed 59 requests\n",
      "Worker  23 completed 58 requests\n",
      "Worker  20 completed 60 requests\n",
      "Worker  56 completed 58 requests\n",
      "Worker  11 completed 59 requests\n",
      "Worker  43 completed 59 requests\n",
      "Worker   3 completed 59 requests\n",
      "Worker  62 completed 59 requests\n",
      "Worker  27 completed 60 requests\n",
      "Worker  54 completed 60 requests\n",
      "Worker   6 completed 59 requests\n",
      "Worker  41 completed 58 requests\n",
      "Worker  32 completed 60 requests\n",
      "Worker  66 completed 58 requests\n",
      "Worker  50 completed 59 requests\n",
      "Worker   9 completed 61 requests\n",
      "Worker  21 completed 59 requests\n",
      "Worker  56 completed 58 requests\n",
      "Worker  11 completed 59 requests\n",
      "Worker  43 completed 59 requests\n",
      "Worker   3 completed 59 requests\n",
      "Worker  62 completed 59 requests\n",
      "Worker  27 completed 60 requests\n",
      "Worker  54 completed 60 requests\n",
      "Worker   6 completed 59 requests\n",
      "Worker  41 completed 58 requests\n",
      "Worker  32 completed 60 requests\n",
      "Worker  66 completed 58 requests\n",
      "Worker  50 completed 59 requests\n",
      "Worker   9 completed 61 requests\n",
      "Worker  21 completed 59 requests\n",
      "Worker   4 completed 62 requests\n",
      "Worker  48 completed 60 requests\n",
      "Worker  15 completed 58 requests\n",
      "Worker  71 completed 59 requests\n",
      "Worker  40 completed 58 requests\n",
      "Worker  42 completed 60 requests\n",
      "Worker   4 completed 62 requests\n",
      "Worker  48 completed 60 requests\n",
      "Worker  15 completed 58 requests\n",
      "Worker  71 completed 59 requests\n",
      "Worker  40 completed 58 requests\n",
      "Worker  42 completed 60 requests\n",
      "Worker  58 completed 59 requests\n",
      "Worker  22 completed 60 requests\n",
      "Worker  30 completed 60 requests\n",
      "Worker   5 completed 59 requests\n",
      "Worker  16 completed 60 requests\n",
      "Worker  61 completed 61 requests\n",
      "Worker  26 completed 60 requests\n",
      "Worker  10 completed 59 requests\n",
      "Worker   1 completed 59 requests\n",
      "Worker  60 completed 60 requests\n",
      "Worker  65 completed 60 requests\n",
      "Worker  39 completed 59 requests\n",
      "Worker  58 completed 59 requests\n",
      "Worker  22 completed 60 requests\n",
      "Worker  30 completed 60 requests\n",
      "Worker   5 completed 59 requests\n",
      "Worker  16 completed 60 requests\n",
      "Worker  61 completed 61 requests\n",
      "Worker  26 completed 60 requests\n",
      "Worker  10 completed 59 requests\n",
      "Worker   1 completed 59 requests\n",
      "Worker  60 completed 60 requests\n",
      "Worker  65 completed 60 requests\n",
      "Worker  39 completed 59 requests\n",
      "Worker  47 completed 57 requests\n",
      "Worker  17 completed 60 requests\n",
      "Worker  74 completed 59 requests\n",
      "Worker  68 completed 62 requests\n",
      "Worker  52 completed 58 requests\n",
      "Worker  75 completed 60 requests\n",
      "Worker  35 completed 58 requests\n",
      "Worker  46 completed 61 requests\n",
      "Worker  47 completed 57 requests\n",
      "Worker  17 completed 60 requests\n",
      "Worker  74 completed 59 requests\n",
      "Worker  68 completed 62 requests\n",
      "Worker  52 completed 58 requests\n",
      "Worker  75 completed 60 requests\n",
      "Worker  35 completed 58 requests\n",
      "Worker  46 completed 61 requests\n",
      "Worker  67 completed 60 requests\n",
      "Worker  19 completed 60 requests\n",
      "Worker  77 completed 59 requests\n",
      "Worker   7 completed 59 requests\n",
      "Worker  38 completed 60 requests\n",
      "Worker  18 completed 59 requests\n",
      "Worker  63 completed 60 requests\n",
      "Worker  69 completed 58 requests\n",
      "Worker  13 completed 59 requests\n",
      "Worker  67 completed 60 requests\n",
      "Worker  19 completed 60 requests\n",
      "Worker  77 completed 59 requests\n",
      "Worker   7 completed 59 requests\n",
      "Worker  38 completed 60 requests\n",
      "Worker  18 completed 59 requests\n",
      "Worker  63 completed 60 requests\n",
      "Worker  69 completed 58 requests\n",
      "Worker  13 completed 59 requests\n",
      "Worker  55 completed 59 requests\n",
      "Worker  70 completed 60 requests\n",
      "Worker  36 completed 59 requests\n",
      "Worker   8 completed 59 requests\n",
      "Worker  76 completed 57 requests\n",
      "Worker  72 completed 59 requests\n",
      "Worker  12 completed 59 requests\n",
      "Worker  53 completed 61 requests\n",
      "Worker  55 completed 59 requests\n",
      "Worker  70 completed 60 requests\n",
      "Worker  36 completed 59 requests\n",
      "Worker   8 completed 59 requests\n",
      "Worker  76 completed 57 requests\n",
      "Worker  72 completed 59 requests\n",
      "Worker  12 completed 59 requests\n",
      "Worker  53 completed 61 requests\n",
      "Worker  78 completed 63 requests\n",
      "Worker  28 completed 61 requests\n",
      "Worker  44 completed 59 requests\n",
      "Worker   0 completed 61 requests\n",
      "Worker  59 completed 60 requests\n",
      "Worker  45 completed 61 requests\n",
      "Worker  37 completed 59 requests\n",
      "Worker  78 completed 63 requests\n",
      "Worker  28 completed 61 requests\n",
      "Worker  44 completed 59 requests\n",
      "Worker   0 completed 61 requests\n",
      "Worker  59 completed 60 requests\n",
      "Worker  45 completed 61 requests\n",
      "Worker  37 completed 59 requests\n",
      "Worker  24 completed 61 requests\n",
      "Worker  73 completed 60 requests\n",
      "Worker   2 completed 58 requests\n",
      "Worker  31 completed 60 requests\n",
      "Worker  64 completed 61 requests\n",
      "Worker  49 completed 58 requests\n",
      "Worker  25 completed 61 requests\n",
      "Worker  51 completed 61 requests\n",
      "Worker  24 completed 61 requests\n",
      "Worker  73 completed 60 requests\n",
      "Worker   2 completed 58 requests\n",
      "Worker  31 completed 60 requests\n",
      "Worker  64 completed 61 requests\n",
      "Worker  49 completed 58 requests\n",
      "Worker  25 completed 61 requests\n",
      "Worker  51 completed 61 requests\n",
      "Worker  34 completed 60 requests\n",
      "Worker  29 completed 59 requests\n",
      "Worker  79 completed 62 requests\n",
      "\n",
      "Load test completed in 122.2s\n",
      "Total requests generated: 4764\n",
      "Load Test Summary:\n",
      "Success rate: 100.0%\n",
      "Average response time: 0.025s\n",
      "P95 response time: 0.047s\n",
      "Scenario distribution:\n",
      "  load_success: 4226 (88.7%)\n",
      "  load_stock_failure: 400 (8.4%)\n",
      "  load_payment_failure: 138 (2.9%)\n",
      "Pausing 10 seconds between phases...\n",
      "Worker  34 completed 60 requests\n",
      "Worker  29 completed 59 requests\n",
      "Worker  79 completed 62 requests\n",
      "\n",
      "Load test completed in 122.2s\n",
      "Total requests generated: 4764\n",
      "Load Test Summary:\n",
      "Success rate: 100.0%\n",
      "Average response time: 0.025s\n",
      "P95 response time: 0.047s\n",
      "Scenario distribution:\n",
      "  load_success: 4226 (88.7%)\n",
      "  load_stock_failure: 400 (8.4%)\n",
      "  load_payment_failure: 138 (2.9%)\n",
      "Pausing 10 seconds between phases...\n",
      "\n",
      "--- Phase 3: Heavy Load (Stress Test) ---\n",
      "Duration: 90s, Virtual Users: 150\n",
      "=== Running Load Test ===\n",
      "Duration: 90s, Virtual Users: 150\n",
      "Expected requests: ~6750 (assuming 0.5 req/s per VU)\n",
      "\n",
      "--- Phase 3: Heavy Load (Stress Test) ---\n",
      "Duration: 90s, Virtual Users: 150\n",
      "=== Running Load Test ===\n",
      "Duration: 90s, Virtual Users: 150\n",
      "Expected requests: ~6750 (assuming 0.5 req/s per VU)\n",
      "Worker 129 completed 44 requests\n",
      "Worker  22 completed 45 requests\n",
      "Worker  73 completed 44 requests\n",
      "Worker  65 completed 43 requests\n",
      "Worker  55 completed 42 requests\n",
      "Worker  15 completed 43 requests\n",
      "Worker 139 completed 43 requests\n",
      "Worker 121 completed 44 requests\n",
      "Worker  92 completed 44 requests\n",
      "Worker 123 completed 43 requests\n",
      "Worker 122 completed 43 requests\n",
      "Worker  44 completed 46 requests\n",
      "Worker 131 completed 44 requests\n",
      "Worker 125 completed 43 requests\n",
      "Worker 129 completed 44 requests\n",
      "Worker  22 completed 45 requests\n",
      "Worker  73 completed 44 requests\n",
      "Worker  65 completed 43 requests\n",
      "Worker  55 completed 42 requests\n",
      "Worker  15 completed 43 requests\n",
      "Worker 139 completed 43 requests\n",
      "Worker 121 completed 44 requests\n",
      "Worker  92 completed 44 requests\n",
      "Worker 123 completed 43 requests\n",
      "Worker 122 completed 43 requests\n",
      "Worker  44 completed 46 requests\n",
      "Worker 131 completed 44 requests\n",
      "Worker 125 completed 43 requests\n",
      "Worker  86 completed 45 requests\n",
      "Worker  24 completed 44 requests\n",
      "Worker  27 completed 43 requests\n",
      "Worker  58 completed 45 requests\n",
      "Worker 147 completed 43 requests\n",
      "Worker  96 completed 44 requests\n",
      "Worker 134 completed 45 requests\n",
      "Worker  69 completed 44 requests\n",
      "Worker 113 completed 42 requests\n",
      "Worker  78 completed 44 requests\n",
      "Worker  18 completed 44 requests\n",
      "Worker  59 completed 43 requests\n",
      "Worker  76 completed 45 requests\n",
      "Worker 149 completed 45 requests\n",
      "Worker  95 completed 44 requests\n",
      "Worker  12 completed 45 requests\n",
      "Worker 137 completed 43 requests\n",
      "Worker  86 completed 45 requests\n",
      "Worker  24 completed 44 requests\n",
      "Worker  27 completed 43 requests\n",
      "Worker  58 completed 45 requests\n",
      "Worker 147 completed 43 requests\n",
      "Worker  96 completed 44 requests\n",
      "Worker 134 completed 45 requests\n",
      "Worker  69 completed 44 requests\n",
      "Worker 113 completed 42 requests\n",
      "Worker  78 completed 44 requests\n",
      "Worker  18 completed 44 requests\n",
      "Worker  59 completed 43 requests\n",
      "Worker  76 completed 45 requests\n",
      "Worker 149 completed 45 requests\n",
      "Worker  95 completed 44 requests\n",
      "Worker  12 completed 45 requests\n",
      "Worker 137 completed 43 requests\n",
      "Worker  66 completed 45 requests\n",
      "Worker  91 completed 42 requests\n",
      "Worker 130 completed 44 requests\n",
      "Worker  67 completed 44 requests\n",
      "Worker  53 completed 45 requests\n",
      "Worker  33 completed 45 requests\n",
      "Worker  43 completed 45 requests\n",
      "Worker  37 completed 44 requests\n",
      "Worker   8 completed 43 requests\n",
      "Worker  25 completed 44 requests\n",
      "Worker  34 completed 45 requests\n",
      "Worker  71 completed 44 requests\n",
      "Worker  49 completed 42 requests\n",
      "Worker 112 completed 44 requests\n",
      "Worker  57 completed 43 requests\n",
      "Worker  36 completed 46 requests\n",
      "Worker   3 completed 44 requests\n",
      "Worker 132 completed 44 requests\n",
      "Worker  66 completed 45 requests\n",
      "Worker  91 completed 42 requests\n",
      "Worker 130 completed 44 requests\n",
      "Worker  67 completed 44 requests\n",
      "Worker  53 completed 45 requests\n",
      "Worker  33 completed 45 requests\n",
      "Worker  43 completed 45 requests\n",
      "Worker  37 completed 44 requests\n",
      "Worker   8 completed 43 requests\n",
      "Worker  25 completed 44 requests\n",
      "Worker  34 completed 45 requests\n",
      "Worker  71 completed 44 requests\n",
      "Worker  49 completed 42 requests\n",
      "Worker 112 completed 44 requests\n",
      "Worker  57 completed 43 requests\n",
      "Worker  36 completed 46 requests\n",
      "Worker   3 completed 44 requests\n",
      "Worker 132 completed 44 requests\n",
      "Worker  63 completed 43 requests\n",
      "Worker  26 completed 44 requests\n",
      "Worker  85 completed 43 requests\n",
      "Worker   2 completed 45 requests\n",
      "Worker 104 completed 44 requests\n",
      "Worker  51 completed 43 requests\n",
      "Worker 128 completed 44 requests\n",
      "Worker  64 completed 44 requests\n",
      "Worker  56 completed 46 requests\n",
      "Worker  79 completed 44 requests\n",
      "Worker  48 completed 44 requests\n",
      "Worker 101 completed 44 requests\n",
      "Worker  75 completed 43 requests\n",
      "Worker  30 completed 45 requests\n",
      "Worker 136 completed 45 requests\n",
      "Worker 145 completed 43 requests\n",
      "Worker  63 completed 43 requests\n",
      "Worker  26 completed 44 requests\n",
      "Worker  85 completed 43 requests\n",
      "Worker   2 completed 45 requests\n",
      "Worker 104 completed 44 requests\n",
      "Worker  51 completed 43 requests\n",
      "Worker 128 completed 44 requests\n",
      "Worker  64 completed 44 requests\n",
      "Worker  56 completed 46 requests\n",
      "Worker  79 completed 44 requests\n",
      "Worker  48 completed 44 requests\n",
      "Worker 101 completed 44 requests\n",
      "Worker  75 completed 43 requests\n",
      "Worker  30 completed 45 requests\n",
      "Worker 136 completed 45 requests\n",
      "Worker 145 completed 43 requests\n",
      "Worker  88 completed 44 requests\n",
      "Worker 117 completed 43 requests\n",
      "Worker   6 completed 46 requests\n",
      "Worker 109 completed 46 requests\n",
      "Worker  98 completed 44 requests\n",
      "Worker  13 completed 44 requests\n",
      "Worker  80 completed 45 requests\n",
      "Worker  82 completed 43 requests\n",
      "Worker 148 completed 42 requests\n",
      "Worker 119 completed 45 requests\n",
      "Worker  74 completed 45 requests\n",
      "Worker 102 completed 46 requests\n",
      "Worker  54 completed 43 requests\n",
      "Worker  93 completed 42 requests\n",
      "Worker   9 completed 44 requests\n",
      "Worker 116 completed 46 requests\n",
      "Worker  88 completed 44 requests\n",
      "Worker 117 completed 43 requests\n",
      "Worker   6 completed 46 requests\n",
      "Worker 109 completed 46 requests\n",
      "Worker  98 completed 44 requests\n",
      "Worker  13 completed 44 requests\n",
      "Worker  80 completed 45 requests\n",
      "Worker  82 completed 43 requests\n",
      "Worker 148 completed 42 requests\n",
      "Worker 119 completed 45 requests\n",
      "Worker  74 completed 45 requests\n",
      "Worker 102 completed 46 requests\n",
      "Worker  54 completed 43 requests\n",
      "Worker  93 completed 42 requests\n",
      "Worker   9 completed 44 requests\n",
      "Worker 116 completed 46 requests\n",
      "Worker  23 completed 44 requests\n",
      "Worker  84 completed 44 requests\n",
      "Worker  21 completed 44 requests\n",
      "Worker  97 completed 44 requests\n",
      "Worker 144 completed 46 requests\n",
      "Worker  20 completed 45 requests\n",
      "Worker  11 completed 45 requests\n",
      "Worker  61 completed 43 requests\n",
      "Worker 114 completed 43 requests\n",
      "Worker  81 completed 44 requests\n",
      "Worker  68 completed 44 requests\n",
      "Worker 146 completed 45 requests\n",
      "Worker  70 completed 46 requests\n",
      "Worker  31 completed 44 requests\n",
      "Worker 141 completed 44 requests\n",
      "Worker  23 completed 44 requests\n",
      "Worker  84 completed 44 requests\n",
      "Worker  21 completed 44 requests\n",
      "Worker  97 completed 44 requests\n",
      "Worker 144 completed 46 requests\n",
      "Worker  20 completed 45 requests\n",
      "Worker  11 completed 45 requests\n",
      "Worker  61 completed 43 requests\n",
      "Worker 114 completed 43 requests\n",
      "Worker  81 completed 44 requests\n",
      "Worker  68 completed 44 requests\n",
      "Worker 146 completed 45 requests\n",
      "Worker  70 completed 46 requests\n",
      "Worker  31 completed 44 requests\n",
      "Worker 141 completed 44 requests\n",
      "Worker  94 completed 45 requests\n",
      "Worker 100 completed 45 requests\n",
      "Worker 124 completed 43 requests\n",
      "Worker  45 completed 44 requests\n",
      "Worker  28 completed 45 requests\n",
      "Worker  10 completed 45 requests\n",
      "Worker  83 completed 43 requests\n",
      "Worker  77 completed 44 requests\n",
      "Worker  90 completed 46 requests\n",
      "Worker  72 completed 44 requests\n",
      "Worker 120 completed 46 requests\n",
      "Worker 143 completed 46 requests\n",
      "Worker  87 completed 45 requests\n",
      "Worker  94 completed 45 requests\n",
      "Worker 100 completed 45 requests\n",
      "Worker 124 completed 43 requests\n",
      "Worker  45 completed 44 requests\n",
      "Worker  28 completed 45 requests\n",
      "Worker  10 completed 45 requests\n",
      "Worker  83 completed 43 requests\n",
      "Worker  77 completed 44 requests\n",
      "Worker  90 completed 46 requests\n",
      "Worker  72 completed 44 requests\n",
      "Worker 120 completed 46 requests\n",
      "Worker 143 completed 46 requests\n",
      "Worker  87 completed 45 requests\n",
      "Worker 105 completed 43 requests\n",
      "Worker 127 completed 44 requests\n",
      "Worker 140 completed 44 requests\n",
      "Worker  40 completed 45 requests\n",
      "Worker 103 completed 43 requests\n",
      "Worker  42 completed 44 requests\n",
      "Worker 135 completed 45 requests\n",
      "Worker 108 completed 44 requests\n",
      "Worker  99 completed 45 requests\n",
      "Worker  16 completed 45 requests\n",
      "Worker 138 completed 45 requests\n",
      "Worker  89 completed 43 requests\n",
      "Worker 105 completed 43 requests\n",
      "Worker 127 completed 44 requests\n",
      "Worker 140 completed 44 requests\n",
      "Worker  40 completed 45 requests\n",
      "Worker 103 completed 43 requests\n",
      "Worker  42 completed 44 requests\n",
      "Worker 135 completed 45 requests\n",
      "Worker 108 completed 44 requests\n",
      "Worker  99 completed 45 requests\n",
      "Worker  16 completed 45 requests\n",
      "Worker 138 completed 45 requests\n",
      "Worker  89 completed 43 requests\n",
      "Worker   0 completed 47 requests\n",
      "Worker 110 completed 44 requests\n",
      "Worker 126 completed 46 requests\n",
      "Worker  14 completed 45 requests\n",
      "Worker  41 completed 44 requests\n",
      "Worker  38 completed 46 requests\n",
      "Worker  32 completed 45 requests\n",
      "Worker  52 completed 45 requests\n",
      "Worker  19 completed 45 requests\n",
      "Worker 111 completed 43 requests\n",
      "Worker   4 completed 47 requests\n",
      "Worker  60 completed 45 requests\n",
      "Worker  17 completed 44 requests\n",
      "Worker   0 completed 47 requests\n",
      "Worker 110 completed 44 requests\n",
      "Worker 126 completed 46 requests\n",
      "Worker  14 completed 45 requests\n",
      "Worker  41 completed 44 requests\n",
      "Worker  38 completed 46 requests\n",
      "Worker  32 completed 45 requests\n",
      "Worker  52 completed 45 requests\n",
      "Worker  19 completed 45 requests\n",
      "Worker 111 completed 43 requests\n",
      "Worker   4 completed 47 requests\n",
      "Worker  60 completed 45 requests\n",
      "Worker  17 completed 44 requests\n",
      "Worker 107 completed 44 requests\n",
      "Worker   5 completed 44 requests\n",
      "Worker 142 completed 44 requests\n",
      "Worker  47 completed 44 requests\n",
      "Worker   7 completed 43 requests\n",
      "Worker  29 completed 44 requests\n",
      "Worker  46 completed 43 requests\n",
      "Worker 107 completed 44 requests\n",
      "Worker   5 completed 44 requests\n",
      "Worker 142 completed 44 requests\n",
      "Worker  47 completed 44 requests\n",
      "Worker   7 completed 43 requests\n",
      "Worker  29 completed 44 requests\n",
      "Worker  46 completed 43 requests\n",
      "Worker   1 completed 44 requests\n",
      "Worker  62 completed 46 requests\n",
      "Worker 115 completed 43 requests\n",
      "Worker 118 completed 44 requests\n",
      "Worker  39 completed 43 requests\n",
      "Worker  35 completed 44 requests\n",
      "Worker  50 completed 46 requests\n",
      "Worker 133 completed 43 requests\n",
      "Worker   1 completed 44 requests\n",
      "Worker  62 completed 46 requests\n",
      "Worker 115 completed 43 requests\n",
      "Worker 118 completed 44 requests\n",
      "Worker  39 completed 43 requests\n",
      "Worker  35 completed 44 requests\n",
      "Worker  50 completed 46 requests\n",
      "Worker 133 completed 43 requests\n",
      "Worker 106 completed 45 requests\n",
      "\n",
      "Load test completed in 92.4s\n",
      "Total requests generated: 6630\n",
      "Load Test Summary:\n",
      "Success rate: 99.9%\n",
      "Average response time: 0.055s\n",
      "P95 response time: 0.129s\n",
      "Scenario distribution:\n",
      "  load_success: 5857 (88.3%)\n",
      "  load_stock_failure: 566 (8.5%)\n",
      "  load_payment_failure: 207 (3.1%)\n",
      "\n",
      "=== Multi-Phase Load Test Summary ===\n",
      "Light phase: 1351 requests\n",
      "Medium phase: 4764 requests\n",
      "Heavy phase: 6630 requests\n",
      "Total load test requests: 12745\n",
      "All tests completed. Total results collected: 12795\n",
      "Worker 106 completed 45 requests\n",
      "\n",
      "Load test completed in 92.4s\n",
      "Total requests generated: 6630\n",
      "Load Test Summary:\n",
      "Success rate: 99.9%\n",
      "Average response time: 0.055s\n",
      "P95 response time: 0.129s\n",
      "Scenario distribution:\n",
      "  load_success: 5857 (88.3%)\n",
      "  load_stock_failure: 566 (8.5%)\n",
      "  load_payment_failure: 207 (3.1%)\n",
      "\n",
      "=== Multi-Phase Load Test Summary ===\n",
      "Light phase: 1351 requests\n",
      "Medium phase: 4764 requests\n",
      "Heavy phase: 6630 requests\n",
      "Total load test requests: 12745\n",
      "All tests completed. Total results collected: 12795\n"
     ]
    }
   ],
   "source": [
    "# Load test with failure injection\n",
    "async def run_load_test(duration_seconds=180, virtual_users=100):\n",
    "    \"\"\"Run load test with failure injection\"\"\"\n",
    "    print(\"=== Running Load Test ===\")\n",
    "    print(f\"Duration: {duration_seconds}s, Virtual Users: {virtual_users}\")\n",
    "    print(f\"Expected requests: ~{duration_seconds * virtual_users // 2} (assuming 0.5 req/s per VU)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + duration_seconds\n",
    "\n",
    "    async def worker(session, worker_id):\n",
    "        \"\"\"Individual worker generating load\"\"\"\n",
    "        worker_results = []\n",
    "        request_count = 0\n",
    "\n",
    "        while time.time() < end_time:\n",
    "            request_count += 1\n",
    "\n",
    "            # Determine pattern (50/50 split)\n",
    "            pattern = 'choreography' if worker_id % 2 == 0 else 'orchestration'\n",
    "\n",
    "            # Failure injection logic\n",
    "            rand_val = random.random()\n",
    "            if rand_val < 0.08:  # 8% stock failure\n",
    "                scenario = 'stock_failure'\n",
    "            elif rand_val < 0.11:  # 3% payment failure (8% + 3%)\n",
    "                scenario = 'payment_failure'\n",
    "            else:\n",
    "                scenario = 'success'\n",
    "\n",
    "            url = ENDPOINTS[pattern]\n",
    "            payload = generate_test_payload(scenario, pattern)\n",
    "\n",
    "            result = await make_request(session, url, payload, pattern, f\"load_{scenario}\")\n",
    "            worker_results.append(result)\n",
    "\n",
    "            # Control request rate (roughly 0.5 requests per second per worker)\n",
    "            await asyncio.sleep(random.uniform(1.5, 2.5))\n",
    "\n",
    "        print(f\"Worker {worker_id:3d} completed {request_count} requests\")\n",
    "        return worker_results\n",
    "\n",
    "    # Run concurrent workers\n",
    "    async with aiohttp.ClientSession(\n",
    "        connector=aiohttp.TCPConnector(limit=virtual_users, limit_per_host=virtual_users//2)\n",
    "    ) as session:\n",
    "\n",
    "        tasks = [worker(session, i) for i in range(virtual_users)]\n",
    "        worker_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        # Flatten results\n",
    "        load_results = []\n",
    "        for worker_result in worker_results:\n",
    "            if isinstance(worker_result, list):\n",
    "                load_results.extend(worker_result)\n",
    "            else:\n",
    "                print(f\"Worker error: {worker_result}\")\n",
    "\n",
    "        test_results.extend(load_results)\n",
    "\n",
    "    actual_duration = time.time() - start_time\n",
    "    print(f\"\\nLoad test completed in {actual_duration:.1f}s\")\n",
    "    print(f\"Total requests generated: {len(load_results)}\")\n",
    "\n",
    "    # Quick analysis\n",
    "    if load_results:\n",
    "        df_load = pd.DataFrame(load_results)\n",
    "        success_rate = (df_load['status_code'].isin([200, 201])).mean() * 100\n",
    "        avg_response_time = df_load['response_time'].mean()\n",
    "        p95_response_time = df_load['response_time'].quantile(0.95)\n",
    "\n",
    "        scenario_counts = df_load['scenario'].value_counts()\n",
    "\n",
    "        print(\"Load Test Summary:\")\n",
    "        print(f\"Success rate: {success_rate:.1f}%\")\n",
    "        print(f\"Average response time: {avg_response_time:.3f}s\")\n",
    "        print(f\"P95 response time: {p95_response_time:.3f}s\")\n",
    "        print(\"Scenario distribution:\")\n",
    "        for scenario, count in scenario_counts.items():\n",
    "            print(f\"  {scenario}: {count} ({count/len(load_results)*100:.1f}%)\")\n",
    "\n",
    "# Run multi-phase load tests (WARNING: This will take ~6 minutes total)\n",
    "print(\"=== Starting Multi-Phase Load Tests ===\")\n",
    "print(\"This will take approximately 6 minutes total (3 phases).\")\n",
    "\n",
    "# Phase 1: Light load (Warm-up)\n",
    "print(\"\\n--- Phase 1: Light Load (Warm-up) ---\")\n",
    "print(\"Duration: 90s, Virtual Users: 30\")\n",
    "light_start = len(test_results)\n",
    "await run_load_test(duration_seconds=90, virtual_users=30)\n",
    "light_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to light load results\n",
    "for i in range(light_start, light_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'light'\n",
    "\n",
    "# Brief pause between phases\n",
    "print(\"Pausing 10 seconds between phases...\")\n",
    "await asyncio.sleep(10)\n",
    "\n",
    "# Phase 2: Medium load (Standard)\n",
    "print(\"\\n--- Phase 2: Medium Load (Standard) ---\")\n",
    "print(\"Duration: 120s, Virtual Users: 80\")\n",
    "medium_start = len(test_results)\n",
    "await run_load_test(duration_seconds=120, virtual_users=80)\n",
    "medium_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to medium load results\n",
    "for i in range(medium_start, medium_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'medium'\n",
    "\n",
    "# Brief pause between phases\n",
    "print(\"Pausing 10 seconds between phases...\")\n",
    "await asyncio.sleep(10)\n",
    "\n",
    "# Phase 3: Heavy load (Stress test)\n",
    "print(\"\\n--- Phase 3: Heavy Load (Stress Test) ---\")\n",
    "print(\"Duration: 90s, Virtual Users: 150\")\n",
    "heavy_start = len(test_results)\n",
    "await run_load_test(duration_seconds=90, virtual_users=150)\n",
    "heavy_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to heavy load results\n",
    "for i in range(heavy_start, heavy_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'heavy'\n",
    "\n",
    "print(\"\\n=== Multi-Phase Load Test Summary ===\")\n",
    "print(f\"Light phase: {light_end - light_start} requests\")\n",
    "print(f\"Medium phase: {medium_end - medium_start} requests\")\n",
    "print(f\"Heavy phase: {len(test_results) - heavy_start} requests\")\n",
    "print(f\"Total load test requests: {len(test_results) - light_start}\")\n",
    "print(f\"All tests completed. Total results collected: {len(test_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "000054d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exporting Performance Data to CSV (robust) ===\n",
      "Ensured output directory exists: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern\n",
      "Events table columns discovered: ['event_id', 'aggregate_id', 'aggregate_type', 'event_type', 'event_data', 'version', 'created_at']\n",
      "Using timestamp column for events queries: created_at\n",
      "Exporting E2E latency data...\n",
      "✓ E2E latency data exported from memory: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern/e2e_latency.csv (12795 rows)\n",
      "Exporting convergence events data...\n",
      "✓ Convergence events exported: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern/convergence_events.csv (51056 rows)\n",
      "Exporting saga steps data (SQL attempt)...\n",
      "✓ Convergence events exported: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern/convergence_events.csv (51056 rows)\n",
      "Exporting saga steps data (SQL attempt)...\n",
      "✓ Saga steps exported from SQL: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern/saga_steps.csv (51056 rows)\n",
      "\n",
      "=== Export Summary (final) ===\n",
      "E2E latency records: 12795\n",
      "Event records: 51056\n",
      "Saga step records: 51056\n",
      "=== CSV Export Results ===\n",
      "E2E latency data: 12795 rows exported\n",
      "Convergence events data: 51056 rows exported\n",
      "Saga steps data: 51056 rows exported\n",
      "✓ Saga steps exported from SQL: /Users/codefox/workspace/practice_infra_arch/saga_pattern/data/orchestration_pattern/saga_steps.csv (51056 rows)\n",
      "\n",
      "=== Export Summary (final) ===\n",
      "E2E latency records: 12795\n",
      "Event records: 51056\n",
      "Saga step records: 51056\n",
      "=== CSV Export Results ===\n",
      "E2E latency data: 12795 rows exported\n",
      "Convergence events data: 51056 rows exported\n",
      "Saga steps data: 51056 rows exported\n"
     ]
    }
   ],
   "source": [
    "# Database aggregation and CSV export (robust refactor with schema inspection)\n",
    "import traceback\n",
    "import numpy as np\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "def export_performance_csvs():\n",
    "    \"\"\"Export performance data to 3 CSV files with robust handling when DB is down and adaptive SQL.\n",
    "\n",
    "    Guarantees: always returns three DataFrames (possibly empty). Does not raise on DB connection errors.\n",
    "    \"\"\"\n",
    "    print(\"=== Exporting Performance Data to CSV (robust) ===\")\n",
    "\n",
    "    csv_dir = Path.cwd() / 'data' / 'orchestration_pattern'\n",
    "    try:\n",
    "        csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Ensured output directory exists: {csv_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create output directory {csv_dir}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Prepare empty DataFrames to return in any failure case\n",
    "    df_e2e = pd.DataFrame()\n",
    "    df_conv = pd.DataFrame()\n",
    "    df_saga = pd.DataFrame()\n",
    "\n",
    "    # Attempt to create engine but don't let failure abort the function\n",
    "    engine = None\n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(CONN_STR)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create SQLAlchemy engine: {e}\")\n",
    "        traceback.print_exc()\n",
    "        engine = None\n",
    "\n",
    "    # Helper to safely execute read_sql_query\n",
    "    def safe_read_sql(sql_text, params=None, parse_dates=None):\n",
    "        nonlocal engine\n",
    "        try:\n",
    "            if engine is None:\n",
    "                raise RuntimeError(\"DB engine is not available\")\n",
    "            return pd.read_sql_query(sql_text, engine, params=params, parse_dates=parse_dates)\n",
    "        except Exception as e:\n",
    "            print(f\"safe_read_sql failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # Discover columns in events table (if DB is available)\n",
    "    events_columns = []\n",
    "    chosen_ts = None\n",
    "    if engine is not None:\n",
    "        try:\n",
    "            inspector = inspect(engine)\n",
    "            if 'events' in inspector.get_table_names():\n",
    "                cols = inspector.get_columns('events')\n",
    "                events_columns = [c['name'] for c in cols]\n",
    "                print(f\"Events table columns discovered: {events_columns}\")\n",
    "\n",
    "                # Prefer processed_at, then common alternatives\n",
    "                candidates = ['processed_at', 'processed_time', 'processed_ts', 'processed_on', 'timestamp', 'event_time', 'created_at', 'occurred_at']\n",
    "                for cand in candidates:\n",
    "                    if cand in events_columns:\n",
    "                        chosen_ts = cand\n",
    "                        break\n",
    "                if chosen_ts:\n",
    "                    print(f\"Using timestamp column for events queries: {chosen_ts}\")\n",
    "                else:\n",
    "                    print(\"No known timestamp column found in events table; convergence/saga queries will be skipped unless fallback from other tables is possible.\")\n",
    "            else:\n",
    "                print(\"No 'events' table found in database (inspector).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to inspect 'events' table: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # If we have in-memory test_results, write e2e from that first\n",
    "    try:\n",
    "        print(\"Exporting E2E latency data...\")\n",
    "        csv_path_e2e = csv_dir / 'e2e_latency.csv'\n",
    "\n",
    "        if 'test_results' in globals() and test_results:\n",
    "            df_raw = pd.DataFrame(test_results)\n",
    "            try:\n",
    "                df_raw[['pattern', 'scenario', 'status_code', 'response_time', 'timestamp', 'load_phase']].to_csv(csv_path_e2e, index=False)\n",
    "                print(f\"✓ E2E latency data exported from memory: {csv_path_e2e} ({len(df_raw)} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write in-memory e2e CSV: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "            df_e2e = df_raw.copy()\n",
    "            if 'response_time' in df_e2e.columns:\n",
    "                df_e2e['e2e_ms'] = df_e2e['response_time'] * 1000\n",
    "        else:\n",
    "            # Fallback to DB\n",
    "            q_e2e = sqlalchemy.text(\"\"\"\n",
    "            SELECT\n",
    "              'orchestration' AS pattern,\n",
    "              CASE\n",
    "                WHEN o.status IN ('CANCELLED', 'FAILED') THEN 'failure'\n",
    "                ELSE 'success'\n",
    "              END AS scenario,\n",
    "              o.order_id,\n",
    "              o.created_at,\n",
    "              COALESCE(o.confirmed_at, o.cancelled_at, o.updated_at) AS finished_at,\n",
    "              TIMESTAMPDIFF(MICROSECOND, o.created_at,\n",
    "                COALESCE(o.confirmed_at, o.cancelled_at, o.updated_at)) / 1000 AS e2e_ms,\n",
    "              NULL AS http_response_time_s\n",
    "            FROM orders o\n",
    "            WHERE o.created_at IS NOT NULL\n",
    "              AND COALESCE(o.confirmed_at, o.cancelled_at, o.updated_at) IS NOT NULL\n",
    "            ORDER BY o.created_at DESC;\n",
    "            \"\"\")\n",
    "            df_e2e = safe_read_sql(q_e2e, parse_dates=['created_at', 'finished_at'])\n",
    "            try:\n",
    "                df_e2e.to_csv(csv_path_e2e, index=False)\n",
    "                print(f\"✓ E2E latency data exported from DB (if any): {csv_path_e2e} ({len(df_e2e)} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write DB-derived e2e CSV: {e}\")\n",
    "                traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during E2E export: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Convergence events\n",
    "    try:\n",
    "        print(\"Exporting convergence events data...\")\n",
    "        csv_path_conv = csv_dir / 'convergence_events.csv'\n",
    "\n",
    "        if chosen_ts is None:\n",
    "            print(\"Skipping convergence events SQL because no timestamp column was determined.\")\n",
    "            df_conv = pd.DataFrame()\n",
    "        else:\n",
    "            q_conv = sqlalchemy.text(f\"\"\"\n",
    "            SELECT aggregate_id, event_type, {chosen_ts} as processed_at\n",
    "            FROM events\n",
    "            WHERE {chosen_ts} IS NOT NULL\n",
    "            ORDER BY aggregate_id, {chosen_ts};\n",
    "            \"\"\")\n",
    "\n",
    "            df_conv = safe_read_sql(q_conv, parse_dates=['processed_at'])\n",
    "            try:\n",
    "                df_conv.to_csv(csv_path_conv, index=False)\n",
    "                print(f\"✓ Convergence events exported: {csv_path_conv} ({len(df_conv)} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write convergence events CSV: {e}\")\n",
    "                traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during convergence export: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Saga steps: try SQL with window functions if chosen_ts is available; otherwise rely on pandas fallback\n",
    "    try:\n",
    "        print(\"Exporting saga steps data (SQL attempt)...\")\n",
    "        csv_path_saga = csv_dir / 'saga_steps.csv'\n",
    "\n",
    "        if chosen_ts is None:\n",
    "            print(\"Skipping saga SQL because no timestamp column was determined. Will rely on pandas fallback (if df_conv available).\")\n",
    "            df_saga = pd.DataFrame()\n",
    "        else:\n",
    "            # Build SQL dynamically using chosen timestamp column\n",
    "            q_saga = sqlalchemy.text(f\"\"\"\n",
    "            WITH step_durations AS (\n",
    "              SELECT\n",
    "                aggregate_id,\n",
    "                event_type,\n",
    "                {chosen_ts} as processed_at,\n",
    "                LAG({chosen_ts}, 1, {chosen_ts}) OVER (PARTITION BY aggregate_id ORDER BY {chosen_ts}) as prev_processed_at\n",
    "              FROM events\n",
    "              WHERE {chosen_ts} IS NOT NULL\n",
    "            )\n",
    "            SELECT\n",
    "              s.aggregate_id AS saga_id,\n",
    "              s.aggregate_id AS order_id,\n",
    "              ROW_NUMBER() OVER (PARTITION BY s.aggregate_id ORDER BY s.processed_at) AS step_number,\n",
    "              s.event_type AS step_name,\n",
    "              CASE\n",
    "                WHEN s.event_type LIKE :cancel OR s.event_type LIKE :fail THEN 'compensation'\n",
    "                ELSE 'forward'\n",
    "              END AS command_type,\n",
    "              'completed' AS status,\n",
    "              s.prev_processed_at AS started_at,\n",
    "              s.processed_at AS completed_at,\n",
    "              TIMESTAMPDIFF(MICROSECOND, s.prev_processed_at, s.processed_at) / 1000 AS duration_ms\n",
    "            FROM step_durations s\n",
    "            ORDER BY s.aggregate_id, s.processed_at;\n",
    "            \"\"\")\n",
    "\n",
    "            df_saga = safe_read_sql(q_saga, params={'cancel': '%Cancel%', 'fail': '%Fail%'}, parse_dates=['started_at', 'completed_at'])\n",
    "\n",
    "            if not df_saga.empty:\n",
    "                try:\n",
    "                    df_saga.to_csv(csv_path_saga, index=False)\n",
    "                    print(f\"✓ Saga steps exported from SQL: {csv_path_saga} ({len(df_saga)} rows)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to write saga_steps CSV from SQL result: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(\"SQL returned no saga rows; will attempt pandas fallback if events exist\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Saga steps SQL attempt failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Pandas fallback: build saga steps from df_conv if df_saga is empty\n",
    "    if (df_saga is None) or (isinstance(df_saga, pd.DataFrame) and df_saga.empty):\n",
    "        try:\n",
    "            if not df_conv.empty:\n",
    "                print(\"Attempting Pandas fallback to build saga steps from convergence events...\")\n",
    "                df_tmp = df_conv.sort_values(['aggregate_id', 'processed_at']).copy()\n",
    "                df_tmp['step_number'] = df_tmp.groupby('aggregate_id').cumcount() + 1\n",
    "                df_tmp['prev_processed_at'] = df_tmp.groupby('aggregate_id')['processed_at'].shift(1)\n",
    "                df_tmp['prev_processed_at'] = df_tmp['prev_processed_at'].fillna(df_tmp['processed_at'])\n",
    "\n",
    "                df_saga = pd.DataFrame({\n",
    "                    'saga_id': df_tmp['aggregate_id'],\n",
    "                    'order_id': df_tmp['aggregate_id'],\n",
    "                    'step_number': df_tmp['step_number'],\n",
    "                    'step_name': df_tmp['event_type'],\n",
    "                    'command_type': np.where(df_tmp['event_type'].str.contains('Cancel|Fail', case=False, na=False), 'compensation', 'forward'),\n",
    "                    'status': 'completed',\n",
    "                    'started_at': df_tmp['prev_processed_at'],\n",
    "                    'completed_at': df_tmp['processed_at'],\n",
    "                    'duration_ms': ((df_tmp['processed_at'] - df_tmp['prev_processed_at']).dt.total_seconds() * 1000).fillna(0)\n",
    "                })\n",
    "\n",
    "                try:\n",
    "                    df_saga.to_csv(csv_path_saga, index=False)\n",
    "                    print(f\"✓ Saga steps exported via Pandas fallback: {csv_path_saga} ({len(df_saga)} rows)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to write saga_steps CSV during pandas fallback: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            else:\n",
    "                print(\"Pandas fallback skipped: no convergence events data (df_conv empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pandas fallback failed unexpectedly: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Final summary and guaranteed return of DataFrames\n",
    "    print(\"\\n=== Export Summary (final) ===\")\n",
    "    print(f\"E2E latency records: {len(df_e2e) if isinstance(df_e2e, pd.DataFrame) else 0}\")\n",
    "    print(f\"Event records: {len(df_conv) if isinstance(df_conv, pd.DataFrame) else 0}\")\n",
    "    print(f\"Saga step records: {len(df_saga) if isinstance(df_saga, pd.DataFrame) else 0}\")\n",
    "\n",
    "    # Ensure non-None DataFrames\n",
    "    df_e2e = df_e2e if isinstance(df_e2e, pd.DataFrame) else pd.DataFrame()\n",
    "    df_conv = df_conv if isinstance(df_conv, pd.DataFrame) else pd.DataFrame()\n",
    "    df_saga = df_saga if isinstance(df_saga, pd.DataFrame) else pd.DataFrame()\n",
    "\n",
    "    return df_e2e, df_conv, df_saga\n",
    "\n",
    "\n",
    "# Export CSV files (robust)\n",
    "df_e2e, df_conv, df_saga = export_performance_csvs()\n",
    "\n",
    "# Check CSV export results\n",
    "print(\"=== CSV Export Results ===\")\n",
    "print(f\"E2E latency data: {len(df_e2e)} rows exported\")\n",
    "print(f\"Convergence events data: {len(df_conv)} rows exported\")\n",
    "print(f\"Saga steps data: {len(df_saga)} rows exported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "332c7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Load Phase Analysis ===\n",
      "Total load test requests: 12745\n",
      "\n",
      "--- LIGHT Load Phase ---\n",
      "  Requests: 1351\n",
      "  Success rate: 100.0%\n",
      "  Avg response time: 0.024s\n",
      "  P95 response time: 0.048s\n",
      "  P99 response time: 0.206s\n",
      "  Scenario distribution:\n",
      "    load_success: 1220 (90.3%)\n",
      "    load_stock_failure: 84 (6.2%)\n",
      "    load_payment_failure: 47 (3.5%)\n",
      "\n",
      "--- MEDIUM Load Phase ---\n",
      "  Requests: 4764\n",
      "  Success rate: 100.0%\n",
      "  Avg response time: 0.025s\n",
      "  P95 response time: 0.047s\n",
      "  P99 response time: 0.392s\n",
      "  Scenario distribution:\n",
      "    load_success: 4226 (88.7%)\n",
      "    load_stock_failure: 400 (8.4%)\n",
      "    load_payment_failure: 138 (2.9%)\n",
      "\n",
      "--- HEAVY Load Phase ---\n",
      "  Requests: 6630\n",
      "  Success rate: 99.9%\n",
      "  Avg response time: 0.055s\n",
      "  P95 response time: 0.129s\n",
      "  P99 response time: 0.732s\n",
      "  Scenario distribution:\n",
      "    load_success: 5857 (88.3%)\n",
      "    load_stock_failure: 566 (8.5%)\n",
      "    load_payment_failure: 207 (3.1%)\n",
      "\n",
      "✓ Load phase results exported: 'orchestration_pattern/load_phase_results.csv' (12745 rows)\n",
      "\n",
      "=== Phase Comparison Summary ===\n",
      "            Count  Avg_RT  Std_RT  P95_RT  P99_RT  Success_Rate\n",
      "load_phase                                                     \n",
      "heavy        6630   0.055   0.135   0.129   0.732        99.879\n",
      "light        1351   0.024   0.034   0.048   0.206       100.000\n",
      "medium       4764   0.025   0.068   0.047   0.392       100.000\n"
     ]
    }
   ],
   "source": [
    "# Multi-phase load test analysis\n",
    "def analyze_load_phases():\n",
    "    \"\"\"Analyze results by load phase\"\"\"\n",
    "    print(\"=== Load Phase Analysis ===\")\n",
    "\n",
    "    # Convert test_results to DataFrame for analysis\n",
    "    df_all = pd.DataFrame(test_results)\n",
    "\n",
    "    if df_all.empty or 'load_phase' not in df_all.columns:\n",
    "        print(\"No load phase data available for analysis.\")\n",
    "        return\n",
    "\n",
    "    # Filter only load test results (exclude single tests)\n",
    "    load_phases = ['light', 'medium', 'heavy']\n",
    "    df_load = df_all[df_all['load_phase'].isin(load_phases)].copy()\n",
    "\n",
    "    if df_load.empty:\n",
    "        print(\"No multi-phase load test data found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total load test requests: {len(df_load)}\")\n",
    "\n",
    "    # Phase-by-phase analysis\n",
    "    for phase in load_phases:\n",
    "        phase_data = df_load[df_load['load_phase'] == phase]\n",
    "        if phase_data.empty:\n",
    "            continue\n",
    "\n",
    "        success_rate = (phase_data['status_code'].isin([200, 201])).mean() * 100\n",
    "        avg_response = phase_data['response_time'].mean()\n",
    "        p95_response = phase_data['response_time'].quantile(0.95)\n",
    "        p99_response = phase_data['response_time'].quantile(0.99)\n",
    "\n",
    "        scenario_dist = phase_data['scenario'].value_counts()\n",
    "\n",
    "        print(f\"\\n--- {phase.upper()} Load Phase ---\")\n",
    "        print(f\"  Requests: {len(phase_data)}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "        print(f\"  Avg response time: {avg_response:.3f}s\")\n",
    "        print(f\"  P95 response time: {p95_response:.3f}s\")\n",
    "        print(f\"  P99 response time: {p99_response:.3f}s\")\n",
    "        print(\"  Scenario distribution:\")\n",
    "        for scenario, count in scenario_dist.items():\n",
    "            print(f\"    {scenario}: {count} ({count/len(phase_data)*100:.1f}%)\")\n",
    "\n",
    "    # Export phase-specific CSV\n",
    "    csv_path = Path.cwd() / 'data' / 'orchestration_pattern' / 'load_phase_results.csv'\n",
    "    df_load.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ Load phase results exported: 'orchestration_pattern/load_phase_results.csv' ({len(df_load)} rows)\")\n",
    "\n",
    "    # Performance comparison across phases\n",
    "    phase_summary = df_load.groupby('load_phase').agg({\n",
    "        'response_time': ['count', 'mean', 'std', lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)],\n",
    "        'status_code': lambda x: (x.isin([200, 201])).mean() * 100\n",
    "    }).round(3)\n",
    "\n",
    "    phase_summary.columns = ['Count', 'Avg_RT', 'Std_RT', 'P95_RT', 'P99_RT', 'Success_Rate']\n",
    "    print(\"\\n=== Phase Comparison Summary ===\")\n",
    "    print(phase_summary)\n",
    "\n",
    "    return df_load\n",
    "\n",
    "# Run phase analysis\n",
    "df_load_phases = analyze_load_phases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b43f6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Raw latency data exported: orchestration_pattern/e2e_latency.csv (12795 rows)\n"
     ]
    }
   ],
   "source": [
    "# Export raw test response times directly to CSV\n",
    "df_raw = pd.DataFrame(test_results)\n",
    "csv_path_raw = Path.cwd() / 'data' / 'orchestration_pattern' / 'e2e_latency.csv'\n",
    "df_raw[['pattern', 'scenario', 'status_code', 'response_time', 'timestamp', 'load_phase']].to_csv(csv_path_raw, index=False)\n",
    "print(f\"✓ Raw latency data exported: orchestration_pattern/e2e_latency.csv ({len(df_raw)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18dc20",
   "metadata": {},
   "source": [
    "# Saga Orchestrator Improvements Summary\n",
    "\n",
    "## Fixed Issues in the 'Insufficient stock' Error Analysis\n",
    "\n",
    "I've implemented three key improvements to the saga orchestrator to address the error you encountered:\n",
    "\n",
    "### 1. Fixed Compensation Loop (Priority 1)\n",
    "**Problem**: Compensation was being run for failed steps, not just completed steps.\n",
    "- **Before**: `for j in range(i, -1, -1):` - included the failed step itself\n",
    "- **After**: `for j in range(i-1, -1, -1):` - only compensates previously completed steps\n",
    "- **Added Safety Check**: Verifies `SagaStepLog.status == COMPLETED` before compensation\n",
    "\n",
    "### 2. Fixed Saga Status Timing (Priority 2)  \n",
    "**Problem**: Saga status was updated before step execution, causing misleading state.\n",
    "- **Before**: Status updated before step execution \n",
    "- **After**: Status updated only after successful step completion\n",
    "- **Benefit**: External status queries now reflect actual completion state\n",
    "\n",
    "### 3. Fixed Multi-Item Inventory Handling (Priority 3)\n",
    "**Problem**: Only first item processed for inventory operations, causing inconsistency.\n",
    "- **Before**: `first_item = payload[\"items\"][0]` - processed only first item\n",
    "- **After**: Loops through all items individually with proper compensation\n",
    "- **Safety**: If any item fails, compensates already-reserved items immediately\n",
    "\n",
    "### What the Original Error Showed\n",
    "- Inventory service correctly returned 400 for insufficient stock\n",
    "- Orchestrator properly triggered compensation flow  \n",
    "- **Issues**: Compensation included failed step + status updated prematurely + multi-item inconsistency\n",
    "\n",
    "### Testing Recommendation\n",
    "Run the existing notebook performance tests to verify the improvements work correctly with both success and failure scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b4d330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Improved Error Handling ===\n",
      "Testing multi-item order with insufficient stock...\n",
      "Payload: {'customer_id': 'customer-001', 'items': [{'book_id': 'book-123', 'quantity': 9999}, {'book_id': 'book-456', 'quantity': 1}]}\n",
      "✓ Saga started: saga-8a6bd4e5\n",
      "Saga Status: FAILED\n",
      "Steps: 1\n",
      "  Step 1: inventory.reserve_stock - FAILED\n",
      "    Error: Service error: 400 - {\"detail\":\"Insufficient stock. Available: 0, Requested: 9999\"}...\n"
     ]
    }
   ],
   "source": [
    "# Test the improved error handling with a specific inventory failure scenario\n",
    "async def test_improved_error_handling():\n",
    "    \"\"\"Test the improved orchestrator with insufficient stock scenario\"\"\"\n",
    "    print(\"=== Testing Improved Error Handling ===\")\n",
    "\n",
    "    # Test with insufficient stock (very high quantity)\n",
    "    test_payload = {\n",
    "        \"customer_id\": \"customer-001\",\n",
    "        \"items\": [\n",
    "            {\"book_id\": \"book-123\", \"quantity\": 9999},  # Will trigger stock failure\n",
    "            {\"book_id\": \"book-456\", \"quantity\": 1}      # This should not be processed if first fails\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        print(\"Testing multi-item order with insufficient stock...\")\n",
    "        print(f\"Payload: {test_payload}\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            async with session.post(\n",
    "                ENDPOINTS['orchestration'],\n",
    "                json=test_payload,\n",
    "                timeout=aiohttp.ClientTimeout(total=30)\n",
    "            ) as response:\n",
    "                response_time = time.time() - start_time\n",
    "\n",
    "                if response.status in [200, 201]:\n",
    "                    result = await response.json()\n",
    "                    saga_id = result.get('saga_id')\n",
    "                    print(f\"✓ Saga started: {saga_id}\")\n",
    "\n",
    "                    # Wait a moment for saga to process\n",
    "                    await asyncio.sleep(2)\n",
    "\n",
    "                    # Check saga status\n",
    "                    async with session.get(f\"http://localhost:8005/saga/{saga_id}/status\") as status_response:\n",
    "                        if status_response.status == 200:\n",
    "                            status_data = await status_response.json()\n",
    "                            print(f\"Saga Status: {status_data['status']}\")\n",
    "                            print(f\"Steps: {len(status_data.get('steps', []))}\")\n",
    "\n",
    "                            for step in status_data.get('steps', []):\n",
    "                                print(f\"  Step {step['step_number']}: {step['step_name']} - {step['status']}\")\n",
    "                                if step.get('error_message'):\n",
    "                                    print(f\"    Error: {step['error_message'][:100]}...\")\n",
    "                else:\n",
    "                    error_text = await response.text()\n",
    "                    print(f\"✗ Request failed: {response.status} - {error_text[:200]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Exception occurred: {str(e)}\")\n",
    "\n",
    "# Run the improved error handling test\n",
    "await test_improved_error_handling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
