{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b44c8",
   "metadata": {},
   "source": [
    "# Analyze Test Results and Generate Report (saga_pattern)\n",
    "\n",
    "This notebook helps analyze test outputs from `saga_pattern` test runs, compute metrics, visualize results, and generate an English report file `saga_pattern/analyze_test_results.md`.\n",
    "\n",
    "Sections:\n",
    "1. Project setup: create directories and English-named report file\n",
    "2. Load test result files (CSV / JSON / logs)\n",
    "3. Parse and normalize filenames and encodings (rename non-ASCII to English)\n",
    "4. Data cleaning and preprocessing\n",
    "5. Compute key metrics and statistical summaries\n",
    "6. Visualizations (time series, histograms, heatmaps)\n",
    "7. Export results and write English markdown report (`saga_pattern/analyze_test_results.md`)\n",
    "8. Unit tests for analysis functions (pytest)\n",
    "9. Execute notebook programmatically and capture outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Project setup - create directories and English-named report file\n",
    "from pathlib import Path\n",
    "\n",
    "saga_dir = Path.cwd()\n",
    "report_path = saga_dir / 'analyze_test_results.md'\n",
    "\n",
    "# Create report file if it doesn't exist\n",
    "if not report_path.exists():\n",
    "    report_path.write_text('# Test Results Analysis\\n\\nReport generated by analyze_test_results.ipynb\\n')\n",
    "\n",
    "print('Report path:', report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c72b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load test result files (CSV / JSON / logs)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "artifacts_dir = saga_dir / 'test_artifacts'\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Discover files (csv, json, txt, log)\n",
    "file_patterns = ['*.csv', '*.json', '*.log', '*.txt']\n",
    "files = []\n",
    "for pat in file_patterns:\n",
    "    files.extend(list(artifacts_dir.glob(pat)))\n",
    "\n",
    "print('Found files:', files)\n",
    "\n",
    "# Load CSV/JSON into DataFrame list\n",
    "frames = []\n",
    "for f in files:\n",
    "    if f.suffix == '.csv':\n",
    "        frames.append(pd.read_csv(f))\n",
    "    elif f.suffix == '.json':\n",
    "        try:\n",
    "            df = pd.read_json(f)\n",
    "        except ValueError:\n",
    "            with open(f, 'r') as fh:\n",
    "                data = json.load(fh)\n",
    "            df = pd.json_normalize(data)\n",
    "        frames.append(df)\n",
    "    else:\n",
    "        # Simple log parser: read lines into DataFrame\n",
    "        with open(f, 'r', encoding='utf-8', errors='replace') as fh:\n",
    "            lines = [l.strip() for l in fh if l.strip()]\n",
    "        df = pd.DataFrame({'raw': lines})\n",
    "        frames.append(df)\n",
    "\n",
    "if frames:\n",
    "    unified = pd.concat(frames, ignore_index=True, sort=False)\n",
    "else:\n",
    "    unified = pd.DataFrame()\n",
    "\n",
    "print('Unified shape:', unified.shape)\n",
    "unified.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Parse and normalize filenames and encodings (rename non-ASCII to English)\n",
    "import os\n",
    "\n",
    "def normalize_name(p: Path) -> Path:\n",
    "    # Simple mapping\n",
    "    mapping = {\n",
    "        'テスト結果を分析する.md': 'analyze_test_results.md'\n",
    "    }\n",
    "    if p.name in mapping:\n",
    "        return p.with_name(mapping[p.name])\n",
    "    # fallback: replace non-ascii with '_'\n",
    "    name = ''.join((c if ord(c) < 128 else '_') for c in p.name)\n",
    "    return p.with_name(name)\n",
    "\n",
    "# Find files with non-ascii name in saga_dir\n",
    "for p in saga_dir.iterdir():\n",
    "    if any(ord(c) >= 128 for c in p.name):\n",
    "        target = normalize_name(p)\n",
    "        try:\n",
    "            p.rename(target)\n",
    "            print(f'Renamed {p.name} -> {target.name}')\n",
    "        except Exception as e:\n",
    "            print('Rename failed:', p, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data cleaning and preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Example cleaning function\n",
    "\n",
    "def clean_unified(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # normalize column names\n",
    "    df.columns = [str(c).strip().lower().replace(' ', '_') for c in df.columns]\n",
    "    # parse timestamps if present\n",
    "    for col in df.columns:\n",
    "        if 'time' in col or 'timestamp' in col or 'date' in col:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            except Exception:\n",
    "                pass\n",
    "    # numeric conversion\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        # try to convert common numeric columns\n",
    "        if any(k in col for k in ('time', 'duration', 'ms', 'sec')):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    # drop rows where everything is NaN\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n",
    "\n",
    "unified = clean_unified(unified)  # reuse unified from earlier cell\n",
    "print('After cleaning shape:', unified.shape)\n",
    "unified.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Compute key metrics and statistical summaries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame) -> dict:\n",
    "    metrics = {}\n",
    "    total = len(df)\n",
    "    metrics['total'] = total\n",
    "    if 'status' in df.columns:\n",
    "        passes = (df['status'].str.lower() == 'success').sum()\n",
    "        metrics['passes'] = int(passes)\n",
    "        metrics['pass_rate'] = passes / total if total else None\n",
    "    # durations\n",
    "    dur_cols = [c for c in df.columns if 'duration' in c or 'response_time' in c or 'ms' in c]\n",
    "    if dur_cols:\n",
    "        d = df[dur_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        metrics['duration_mean'] = d.mean(numeric_only=True).to_dict()\n",
    "        metrics['duration_median'] = d.median(numeric_only=True).to_dict()\n",
    "    # failure counts by scenario\n",
    "    if 'scenario' in df.columns:\n",
    "        metrics['failure_counts'] = df[df['status'].str.lower() != 'success']['scenario'].value_counts().to_dict()\n",
    "    return metrics\n",
    "\n",
    "metrics = compute_metrics(unified)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Visualizations (time series, histograms, heatmaps)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series of failures\n",
    "if 'timestamp' in unified.columns or 'created_at' in unified.columns:\n",
    "    time_col = 'timestamp' if 'timestamp' in unified.columns else 'created_at'\n",
    "    df_time = unified.copy()\n",
    "    df_time[time_col] = pd.to_datetime(df_time[time_col], errors='coerce')\n",
    "    df_time = df_time.dropna(subset=[time_col])\n",
    "    df_time['date'] = df_time[time_col].dt.floor('D')\n",
    "    daily = df_time.groupby(['date', 'status']).size().unstack(fill_value=0)\n",
    "    if not daily.empty:\n",
    "        daily.plot(kind='line', figsize=(10,4))\n",
    "        plt.title('Daily test counts by status')\n",
    "        plt.show()\n",
    "\n",
    "# Histogram of response_time\n",
    "if 'response_time' in unified.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(unified['response_time'].dropna(), kde=False)\n",
    "    plt.title('Response Time Distribution')\n",
    "    plt.xlabel('seconds')\n",
    "    plt.show()\n",
    "\n",
    "# Heatmap example: failures by scenario vs status (pivot)\n",
    "if 'scenario' in unified.columns and 'status' in unified.columns:\n",
    "    pivot = unified.pivot_table(index='scenario', columns='status', aggfunc='size', fill_value=0)\n",
    "    if not pivot.empty:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(pivot, annot=True, fmt='d', cmap='Reds')\n",
    "        plt.title('Failures by Scenario and Status')\n",
    "        plt.show()\n",
    "\n",
    "# Save figures path\n",
    "figs_dir = saga_dir / 'analysis_figs'\n",
    "figs_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(figs_dir / 'last_plot.png')\n",
    "print('Saved example fig to', figs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Export results and write English markdown report (saga_pattern/analyze_test_results.md)\n",
    "report_md = saga_dir / 'analyze_test_results.md'\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append('# Test Results Analysis')\n",
    "report_lines.append('')\n",
    "report_lines.append('## Summary Metrics')\n",
    "for k, v in metrics.items():\n",
    "    report_lines.append(f'- {k}: {v}')\n",
    "\n",
    "# attach example figure\n",
    "last_fig = figs_dir / 'last_plot.png'\n",
    "if last_fig.exists():\n",
    "    report_lines.append('\\n## Figures')\n",
    "    report_lines.append(f'![]({last_fig.name})')\n",
    "\n",
    "report_md.write_text('\\n'.join(report_lines), encoding='utf-8')\n",
    "print('Wrote report to', report_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dd6b0",
   "metadata": {},
   "source": [
    "## Section 8: Unit tests (pytest)\n",
    "\n",
    "Create tests under `tests/` for parsing, cleaning, and metric functions. Example test snippet:\n",
    "\n",
    "```python\n",
    "# tests/test_analysis.py\n",
    "import pandas as pd\n",
    "from analyze_test_results import clean_unified, compute_metrics\n",
    "\n",
    "def test_clean_unified_basic():\n",
    "    df = pd.DataFrame({'status': ['success', 'failure'], 'response_time': ['0.1', '0.2']})\n",
    "    out = clean_unified(df)\n",
    "    assert 'response_time' in out.columns\n",
    "\n",
    "def test_compute_metrics_counts():\n",
    "    df = pd.DataFrame({'status': ['success','failure','success']})\n",
    "    m = compute_metrics(df)\n",
    "    assert m['total'] == 3\n",
    "    assert m['passes'] == 2\n",
    "```\n",
    "\n",
    "Run tests with:\n",
    "\n",
    "```bash\n",
    "pytest -q\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad553c5",
   "metadata": {},
   "source": [
    "## Section 9: Execute notebook programmatically and capture outputs\n",
    "\n",
    "You can run this notebook non-interactively with nbconvert or papermill. Examples:\n",
    "\n",
    "```bash\n",
    "# Run in-place with nbconvert\n",
    "jupyter nbconvert --to notebook --execute analyze_test_results.ipynb --output analyze_test_results.executed.ipynb\n",
    "\n",
    "# Using papermill (parameterize if needed)\n",
    "papermill analyze_test_results.ipynb analyze_test_results.executed.ipynb\n",
    "```\n",
    "\n",
    "Capture stderr/stdout to files when running the commands in a shell.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
