{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae94247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports are ready in one place.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import socket\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import sqlalchemy\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import random\n",
    "import socket\n",
    "\n",
    "print('All imports are ready in one place.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc701220",
   "metadata": {},
   "source": [
    "# Step 1: Pre-check and Environment Setup\n",
    "\n",
    "This cell checks for Docker, determines the correct `docker-compose` command, and verifies that no conflicting containers are running. This is a safety check to prevent errors when starting the services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67aad66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for service connectivity\n",
    "def is_port_open(host: str, port: int, timeout: float = 1.0) -> bool:\n",
    "    \"\"\"Check if a TCP port is open on the given host.\"\"\"\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=timeout):\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check endpoints\n",
    "HEALTH_ENDPOINTS = [\n",
    "    (\"single_pessimistic\", \"order-service\", 'http://localhost:8000/health'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e05254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Service Health Check ===\n",
      "single_pessimistic   | order-service   | ✓\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick health check for services\n",
    "print(\"=== Service Health Check ===\")\n",
    "async def quick_health_check():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for pattern, service_name, health_url in HEALTH_ENDPOINTS:\n",
    "            try:\n",
    "                async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=5)) as response:\n",
    "                    status = \"✓\" if response.status == 200 else f\"✗ {response.status}\"\n",
    "                    print(f\"{pattern:20} | {service_name:15} | {status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{pattern:20} | {service_name:15} | ✗ {str(e)[:30]}...\")\n",
    "\n",
    "await quick_health_check()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved utility function to generate test data with better failure injection\n",
    "def generate_test_payload_improved(scenario='success', pattern='single_pessimistic'):\n",
    "    \"\"\"Generate test payload with improved failure injection logic\"\"\"\n",
    "    # Use valid customer and book IDs that exist in the database\n",
    "    valid_customers = [\"customer-001\", \"customer-002\", \"customer-003\", \"customer-004\", \"customer-005\"]\n",
    "    # Use only books with sufficient stock to avoid stock failures in success cases\n",
    "    cheap_books = [\"book-123\", \"book-789\"]  # Books under 4000 yen with 100 stock\n",
    "    expensive_books = [\"book-456\"]  # Books over 5000 yen for payment failure\n",
    "    low_stock_books = [\"book-101\", \"book-202\"]  # Books with 50 stock for stock failure tests\n",
    "\n",
    "    base_customer = random.choice(valid_customers)\n",
    "\n",
    "    # Failure injection logic\n",
    "    if scenario == 'stock_failure':\n",
    "        # Use books with limited stock to trigger stock failure\n",
    "        base_book = random.choice(low_stock_books)\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 60}]  # More than available stock (50)\n",
    "        }\n",
    "    elif scenario == 'payment_failure':\n",
    "        # Use expensive book to trigger payment failure (amount > 5000)\n",
    "        base_book = random.choice(expensive_books)\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 1}]  # 8000 > 5000\n",
    "        }\n",
    "    else:\n",
    "        # Normal success case - use cheap books with plenty of stock\n",
    "        base_book = random.choice(cheap_books)\n",
    "        return {\n",
    "            \"customer_id\": base_customer,\n",
    "            \"items\": [{\"book_id\": base_book, \"quantity\": 1}]\n",
    "        }\n",
    "\n",
    "# Replace the original function (without parentheses - assign the function, not call it)\n",
    "generate_test_payload = generate_test_payload_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe531ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async HTTP client utilities\n",
    "async def make_request(session, url, payload, pattern, scenario):\n",
    "    \"\"\"Make async HTTP request and record timing\"\"\"\n",
    "    request_id = uuid.uuid4().hex[:8]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "            response_time = time.time() - start_time\n",
    "            if response.status in (200, 201):\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    order_id = result.get('order_id', result.get('id', request_id))\n",
    "                except Exception:\n",
    "                    order_id = request_id\n",
    "                    result = await response.text()\n",
    "            else:\n",
    "                order_id = request_id\n",
    "                result = await response.text()\n",
    "            return {\n",
    "                'pattern': pattern,\n",
    "                'scenario': scenario,\n",
    "                'order_id': order_id,\n",
    "                'request_id': request_id,\n",
    "                'status_code': response.status,\n",
    "                'response_time': response_time,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'result': str(result)[:200],\n",
    "                'load_phase': 'single'\n",
    "            }\n",
    "    except Exception as e:\n",
    "        response_time = time.time() - start_time\n",
    "        return {\n",
    "            'pattern': pattern,\n",
    "            'scenario': scenario,\n",
    "            'order_id': request_id,\n",
    "            'request_id': request_id,\n",
    "            'status_code': 'ERROR',\n",
    "            'response_time': response_time,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'result': str(e)[:200],\n",
    "            'load_phase': 'single'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69df536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous test results cleared. Ready for new tests.\n"
     ]
    }
   ],
   "source": [
    "# Clear previous test results to start fresh\n",
    "test_results = []\n",
    "print(\"Previous test results cleared. Ready for new tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09540202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Updated Failure Injection ===\n",
      "\n",
      "1. Testing success case:\n",
      "   Payload: {'customer_id': 'customer-001', 'items': [{'book_id': 'book-123', 'quantity': 1}]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ENDPOINTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m make_request(session, ENDPOINTS[\u001b[33m'\u001b[39m\u001b[33msingle_pessimistic\u001b[39m\u001b[33m'\u001b[39m], payload, \u001b[33m'\u001b[39m\u001b[33msingle_pessimistic\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpayment_failure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mresponse_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_failure_injection()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_failure_injection\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m payload = generate_test_payload(\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Payload: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpayload\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m make_request(session, \u001b[43mENDPOINTS\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msingle_pessimistic\u001b[39m\u001b[33m'\u001b[39m], payload, \u001b[33m'\u001b[39m\u001b[33msingle_pessimistic\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mresponse_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Test 2: Stock failure case\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ENDPOINTS' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the updated failure injection logic with a few manual tests\n",
    "async def test_failure_injection():\n",
    "    \"\"\"Quick test of the updated failure injection logic\"\"\"\n",
    "    print(\"=== Testing Updated Failure Injection ===\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Test 1: Success case\n",
    "        print(\"\\n1. Testing success case:\")\n",
    "        payload = generate_test_payload('success')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['single_pessimistic'], payload, 'single_pessimistic', 'success')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "        # Test 2: Stock failure case\n",
    "        print(\"\\n2. Testing stock failure (high quantity):\")\n",
    "        payload = generate_test_payload('stock_failure')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['single_pessimistic'], payload, 'single_pessimistic', 'stock_failure')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "        # Test 3: Payment failure case\n",
    "        print(\"\\n3. Testing payment failure (high amount):\")\n",
    "        payload = generate_test_payload('payment_failure')\n",
    "        print(f\"   Payload: {payload}\")\n",
    "        result = await make_request(session, ENDPOINTS['single_pessimistic'], payload, 'single_pessimistic', 'payment_failure')\n",
    "        print(f\"   Result: {result['status_code']} - {result['response_time']:.3f}s\")\n",
    "\n",
    "await test_failure_injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6193c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-shot and abnormal case tests\n",
    "async def run_single_tests():\n",
    "    \"\"\"Run single-shot tests for functional verification\"\"\"\n",
    "    print(\"=== Running Single-shot Tests ===\")\n",
    "\n",
    "    test_cases = [\n",
    "        # Normal success cases (5 times each)\n",
    "        ('single_pessimistic', 'success', 5),\n",
    "\n",
    "        # Failure cases (10 times each)\n",
    "        ('single_pessimistic', 'stock_failure', 10),\n",
    "        ('single_pessimistic', 'payment_failure', 10),\n",
    "    ]\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for pattern, scenario, count in test_cases:\n",
    "            print(f\"\\nTesting {pattern} - {scenario} ({count} times)\")\n",
    "            url = ENDPOINTS[pattern]\n",
    "\n",
    "            for i in range(count):\n",
    "                payload = generate_test_payload(scenario, pattern)\n",
    "                result = await make_request(session, url, payload, pattern, scenario)\n",
    "                test_results.append(result)\n",
    "\n",
    "                status_symbol = \"✓\" if result['status_code'] in [200, 201] else \"✗\"\n",
    "                print(f\"  {i+1:2d}. {status_symbol} {result['status_code']} - {result['response_time']:.3f}s - {result['order_id']}\")\n",
    "\n",
    "                # Brief delay between requests\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "# Run single tests\n",
    "await run_single_tests()\n",
    "\n",
    "print(f\"\\nSingle tests completed. Total results: {len(test_results)}\")\n",
    "\n",
    "# Show summary\n",
    "df_single = pd.DataFrame(test_results)\n",
    "if not df_single.empty:\n",
    "    summary = df_single.groupby(['pattern', 'scenario']).agg({\n",
    "        'response_time': ['count', 'mean', 'std'],\n",
    "        'status_code': lambda x: (x.isin([200, 201])).sum()\n",
    "    }).round(3)\n",
    "    print(\"\\nSingle Test Summary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check test results summary\n",
    "print(\"=== Test Results Summary ===\")\n",
    "print(f\"Total test results: {len(test_results)}\")\n",
    "\n",
    "if test_results:\n",
    "    df = pd.DataFrame(test_results)\n",
    "    success_count = (df['status_code'].isin([200, 201])).sum()\n",
    "    error_count = (df['status_code'] == 'ERROR').sum()\n",
    "    print(f\"Successful requests: {success_count}\")\n",
    "    print(f\"Error requests: {error_count}\")\n",
    "    print(f\"Success rate: {success_count / len(test_results) * 100:.1f}%\")\n",
    "\n",
    "    if error_count > 0:\n",
    "        print(\"\\nSample error details:\")\n",
    "        error_results = df[df['status_code'] == 'ERROR'].head(3)\n",
    "        for _, row in error_results.iterrows():\n",
    "            print(f\"  {row['pattern']} - {row['scenario']}: {row['result'][:100]}...\")\n",
    "\n",
    "    print(\"\\nStatus code distribution:\")\n",
    "    print(df['status_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528501ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test with failure injection\n",
    "async def run_load_test(duration_seconds=180, virtual_users=100):\n",
    "    \"\"\"Run load test with failure injection\"\"\"\n",
    "    print(\"=== Running Load Test ===\")\n",
    "    print(f\"Duration: {duration_seconds}s, Virtual Users: {virtual_users}\")\n",
    "    print(f\"Expected requests: ~{duration_seconds * virtual_users // 2} (assuming 0.5 req/s per VU)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + duration_seconds\n",
    "\n",
    "    async def worker(session, worker_id):\n",
    "        \"\"\"Individual worker generating load\"\"\"\n",
    "        worker_results = []\n",
    "        request_count = 0\n",
    "\n",
    "        while time.time() < end_time:\n",
    "            request_count += 1\n",
    "\n",
    "            # Determine pattern (only single_pessimistic)\n",
    "            pattern = 'single_pessimistic'\n",
    "\n",
    "            # Failure injection logic\n",
    "            rand_val = random.random()\n",
    "            if rand_val < 0.08:  # 8% stock failure\n",
    "                scenario = 'stock_failure'\n",
    "            elif rand_val < 0.11:  # 3% payment failure (8% + 3%)\n",
    "                scenario = 'payment_failure'\n",
    "            else:\n",
    "                scenario = 'success'\n",
    "\n",
    "            url = ENDPOINTS[pattern]\n",
    "            payload = generate_test_payload(scenario, pattern)\n",
    "\n",
    "            result = await make_request(session, url, payload, pattern, f\"load_{scenario}\")\n",
    "            worker_results.append(result)\n",
    "\n",
    "            # Control request rate (roughly 0.5 requests per second per worker)\n",
    "            await asyncio.sleep(random.uniform(1.5, 2.5))\n",
    "\n",
    "        print(f\"Worker {worker_id:3d} completed {request_count} requests\")\n",
    "        return worker_results\n",
    "\n",
    "    # Run concurrent workers\n",
    "    async with aiohttp.ClientSession(\n",
    "        connector=aiohttp.TCPConnector(limit=virtual_users, limit_per_host=virtual_users//2)\n",
    "    ) as session:\n",
    "\n",
    "        tasks = [worker(session, i) for i in range(virtual_users)]\n",
    "        worker_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        # Flatten results\n",
    "        load_results = []\n",
    "        for worker_result in worker_results:\n",
    "            if isinstance(worker_result, list):\n",
    "                load_results.extend(worker_result)\n",
    "            else:\n",
    "                print(f\"Worker error: {worker_result}\")\n",
    "\n",
    "        test_results.extend(load_results)\n",
    "\n",
    "    actual_duration = time.time() - start_time\n",
    "    print(f\"\\nLoad test completed in {actual_duration:.1f}s\")\n",
    "    print(f\"Total requests generated: {len(load_results)}\")\n",
    "\n",
    "    # Quick analysis\n",
    "    if load_results:\n",
    "        df_load = pd.DataFrame(load_results)\n",
    "        success_rate = (df_load['status_code'].isin([200, 201])).mean() * 100\n",
    "        avg_response_time = df_load['response_time'].mean()\n",
    "        p95_response_time = df_load['response_time'].quantile(0.95)\n",
    "\n",
    "        scenario_counts = df_load['scenario'].value_counts()\n",
    "\n",
    "        print(\"Load Test Summary:\")\n",
    "        print(f\"Success rate: {success_rate:.1f}%\")\n",
    "        print(f\"Average response time: {avg_response_time:.3f}s\")\n",
    "        print(f\"P95 response time: {p95_response_time:.3f}s\")\n",
    "        print(\"Scenario distribution:\")\n",
    "        for scenario, count in scenario_counts.items():\n",
    "            print(f\"  {scenario}: {count} ({count/len(load_results)*100:.1f}%)\")\n",
    "\n",
    "# Run multi-phase load tests (WARNING: This will take ~6 minutes total)\n",
    "print(\"=== Starting Multi-Phase Load Tests ===\")\n",
    "print(\"This will take approximately 6 minutes total (3 phases).\")\n",
    "\n",
    "# Phase 1: Light load (Warm-up)\n",
    "print(\"\\n--- Phase 1: Light Load (Warm-up) ---\")\n",
    "print(\"Duration: 90s, Virtual Users: 30\")\n",
    "light_start = len(test_results)\n",
    "await run_load_test(duration_seconds=90, virtual_users=30)\n",
    "light_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to light load results\n",
    "for i in range(light_start, light_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'light'\n",
    "\n",
    "# Brief pause between phases\n",
    "print(\"Pausing 10 seconds between phases...\")\n",
    "await asyncio.sleep(10)\n",
    "\n",
    "# Phase 2: Medium load (Standard)\n",
    "print(\"\\n--- Phase 2: Medium Load (Standard) ---\")\n",
    "print(\"Duration: 120s, Virtual Users: 80\")\n",
    "medium_start = len(test_results)\n",
    "await run_load_test(duration_seconds=120, virtual_users=80)\n",
    "medium_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to medium load results\n",
    "for i in range(medium_start, medium_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'medium'\n",
    "\n",
    "# Brief pause between phases\n",
    "print(\"Pausing 10 seconds between phases...\")\n",
    "await asyncio.sleep(10)\n",
    "\n",
    "# Phase 3: Heavy load (Stress test)\n",
    "print(\"\\n--- Phase 3: Heavy Load (Stress Test) ---\")\n",
    "print(\"Duration: 90s, Virtual Users: 150\")\n",
    "heavy_start = len(test_results)\n",
    "await run_load_test(duration_seconds=90, virtual_users=150)\n",
    "heavy_end = len(test_results)\n",
    "\n",
    "# Add phase identifier to heavy load results\n",
    "for i in range(heavy_start, heavy_end):\n",
    "    if i < len(test_results):\n",
    "        test_results[i]['load_phase'] = 'heavy'\n",
    "\n",
    "print(\"\\n=== Multi-Phase Load Test Summary ===\")\n",
    "print(f\"Light phase: {light_end - light_start} requests\")\n",
    "print(f\"Medium phase: {medium_end - medium_start} requests\")\n",
    "print(f\"Heavy phase: {len(test_results) - heavy_start} requests\")\n",
    "print(f\"Total load test requests: {len(test_results) - light_start}\")\n",
    "print(f\"All tests completed. Total results collected: {len(test_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database aggregation and CSV export\n",
    "def export_performance_csvs():\n",
    "    \"\"\"Export performance data to 3 CSV files\"\"\"\n",
    "    print(\"=== Exporting Performance Data to CSV ===\")\n",
    "\n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(CONN_STR)\n",
    "\n",
    "        # 1. E2E latency CSV (Raw test response times)\n",
    "        print(\"Exporting E2E latency data...\")\n",
    "        if 'test_results' in globals() and test_results:\n",
    "            df_raw = pd.DataFrame(test_results)\n",
    "            csv_path_e2e = Path.cwd() / 'data' / 'single_pessimistic_pattern' / 'e2e_latency.csv'\n",
    "            df_raw[['pattern','scenario','status_code','response_time','timestamp','load_phase']].to_csv(csv_path_e2e, index=False)\n",
    "            print(f\"✓ E2E latency data exported: single_pessimistic_pattern/e2e_latency.csv ({len(df_raw)} rows)\")\n",
    "            # Create df_e2e for summary\n",
    "            df_e2e = df_raw.copy()\n",
    "            df_e2e['e2e_ms'] = df_e2e['response_time'] * 1000  # seconds to ms\n",
    "        else:\n",
    "            # Fallback to database query\n",
    "            q_e2e = sqlalchemy.text(\"\"\"\n",
    "            SELECT\n",
    "              'single_pessimistic' AS pattern,\n",
    "              CASE WHEN o.status IN ('CANCELLED','FAILED') THEN 'failure' ELSE 'success' END AS scenario,\n",
    "              o.order_id,\n",
    "              o.created_at,\n",
    "              COALESCE(o.confirmed_at,o.cancelled_at,o.updated_at) AS finished_at,\n",
    "              TIMESTAMPDIFF(MICROSECOND,o.created_at,COALESCE(o.confirmed_at,o.cancelled_at,o.updated_at)) / 1000 AS e2e_ms,\n",
    "              NULL AS http_response_time_s\n",
    "            FROM orders o\n",
    "            WHERE o.created_at IS NOT NULL\n",
    "              AND COALESCE(o.confirmed_at,o.cancelled_at,o.updated_at) IS NOT NULL\n",
    "            ORDER BY o.created_at DESC;\n",
    "            \"\"\")\n",
    "            df_e2e = pd.read_sql_query(q_e2e, engine, parse_dates=['created_at','finished_at'])\n",
    "            csv_path_e2e = Path.cwd() / 'data' / 'single_pessimistic_pattern' / 'e2e_latency.csv'\n",
    "            df_e2e.to_csv(csv_path_e2e, index=False)\n",
    "            print(f\"✓ E2E latency data exported: single_pessimistic_pattern/e2e_latency.csv ({len(df_e2e)} rows)\")\n",
    "\n",
    "        # 2. Convergence Events CSV\n",
    "        print(\"Exporting convergence events data...\")\n",
    "        q_conv = sqlalchemy.text(\"\"\"\n",
    "        SELECT aggregate_id,event_type,created_at as processed_at\n",
    "        FROM events\n",
    "        WHERE created_at IS NOT NULL\n",
    "        ORDER BY aggregate_id,created_at;\n",
    "        \"\"\")\n",
    "        df_conv = pd.read_sql_query(q_conv, engine, parse_dates=['processed_at'])\n",
    "        csv_path_conv = Path.cwd() / 'data' / 'single_pessimistic_pattern' / 'convergence_events.csv'\n",
    "        df_conv.to_csv(csv_path_conv, index=False)\n",
    "        print(f\"✓ Convergence events exported: single_pessimistic_pattern/convergence_events.csv ({len(df_conv)} rows)\")\n",
    "\n",
    "        # 3. Saga Steps CSV\n",
    "        print(\"Exporting saga steps data...\")\n",
    "        q_saga = sqlalchemy.text(\"\"\"\n",
    "        WITH step_durations AS (\n",
    "          SELECT aggregate_id,event_type,created_at as processed_at,\n",
    "            LAG(created_at,1,created_at) OVER (PARTITION BY aggregate_id ORDER BY created_at) as prev_processed_at\n",
    "          FROM events WHERE created_at IS NOT NULL\n",
    "        )\n",
    "        SELECT\n",
    "          s.aggregate_id AS saga_id,\n",
    "          s.aggregate_id AS order_id,\n",
    "          ROW_NUMBER() OVER (PARTITION BY s.aggregate_id ORDER BY s.processed_at) AS step_number,\n",
    "          s.event_type AS step_name,\n",
    "          CASE WHEN s.event_type LIKE :cancel OR s.event_type LIKE :fail THEN 'compensation' ELSE 'forward' END AS command_type,\n",
    "          'completed' AS status,\n",
    "          s.prev_processed_at AS started_at,\n",
    "          s.processed_at AS completed_at,\n",
    "          TIMESTAMPDIFF(MICROSECOND,s.prev_processed_at,s.processed_at)/1000 AS duration_ms\n",
    "        FROM step_durations s ORDER BY s.aggregate_id,s.processed_at;\n",
    "        \"\"\")\n",
    "        df_saga = pd.read_sql_query(q_saga, engine, params={'cancel':'%Cancel%','fail':'%Fail%'}, parse_dates=['started_at','completed_at'])\n",
    "        csv_path_saga = Path.cwd() / 'data' / 'single_pessimistic_pattern' / 'saga_steps.csv'\n",
    "        df_saga.to_csv(csv_path_saga, index=False)\n",
    "        print(f\"✓ Saga steps exported: single_pessimistic_pattern/saga_steps.csv ({len(df_saga)} rows)\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n=== Export Summary ===\")\n",
    "        print(f\"E2E latency records: {len(df_e2e)}\")\n",
    "        print(f\"Event records: {len(df_conv)}\")\n",
    "        print(f\"Saga step records: {len(df_saga)}\")\n",
    "        if len(df_e2e)>0:\n",
    "            print(\"\\nE2E Latency Summary:\")\n",
    "            print(f\"  p50: {df_e2e['e2e_ms'].quantile(0.5):.1f}ms\")\n",
    "            print(f\"  p95: {df_e2e['e2e_ms'].quantile(0.95):.1f}ms\")\n",
    "            print(f\"  p99: {df_e2e['e2e_ms'].quantile(0.99):.1f}ms\")\n",
    "        if len(df_conv)>0:\n",
    "            convergence_summary = df_conv.groupby('aggregate_id').agg({'processed_at':['min','max','count']}).reset_index()\n",
    "            convergence_summary.columns=['aggregate_id','first_event','last_event','event_count']\n",
    "            convergence_summary['convergence_s']=(convergence_summary['last_event']-convergence_summary['first_event']).dt.total_seconds()\n",
    "            print(\"\\nConvergence Time Summary:\")\n",
    "            print(f\"  Average: {convergence_summary['convergence_s'].mean():.2f}s\")\n",
    "            print(f\"  p95: {convergence_summary['convergence_s'].quantile(0.95):.2f}s\")\n",
    "        if len(df_saga)>0:\n",
    "            compensation_count=df_saga['command_type'].str.contains('compensation').sum()\n",
    "            print(\"\\nSaga Steps Summary:\")\n",
    "            print(f\"  Total steps: {len(df_saga)}\")\n",
    "            print(f\"  Compensation steps: {compensation_count}\")\n",
    "            print(f\"  Compensation rate: {compensation_count/len(df_saga)*100:.1f}%\")\n",
    "        return df_e2e, df_conv, df_saga\n",
    "    except Exception as e:\n",
    "        print(f\"Error during CSV export: {e}\")\n",
    "        print(\"Note: Ensure database services are running and tables exist.\")\n",
    "        return None, None, None\n",
    "\n",
    "# Export CSV files\n",
    "df_e2e, df_conv, df_saga = export_performance_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export load phase results to CSV\n",
    "print(\"=== Exporting Load Phase Results to CSV ===\")\n",
    "\n",
    "try:\n",
    "    if 'test_results' in globals() and test_results:\n",
    "        df_load_phase = pd.DataFrame(test_results)\n",
    "\n",
    "        # Filter only load test results (exclude single tests)\n",
    "        load_results = df_load_phase[df_load_phase['load_phase'] != 'single']\n",
    "\n",
    "        if not load_results.empty:\n",
    "            # Group by load_phase and scenario\n",
    "            load_phase_summary = load_results.groupby(['load_phase', 'scenario']).agg({\n",
    "                'response_time': ['count', 'mean', 'std', 'min', 'max', lambda x: x.quantile(0.95), lambda x: x.quantile(0.99)],\n",
    "                'status_code': lambda x: (x.isin([200, 201])).sum()\n",
    "            }).round(3)\n",
    "\n",
    "            # Flatten column names\n",
    "            load_phase_summary.columns = ['count', 'mean_rt', 'std_rt', 'min_rt', 'max_rt', 'p95_rt', 'p99_rt', 'success_count']\n",
    "            load_phase_summary = load_phase_summary.reset_index()\n",
    "\n",
    "            # Calculate success rate\n",
    "            load_phase_summary['success_rate'] = (load_phase_summary['success_count'] / load_phase_summary['count'] * 100).round(1)\n",
    "\n",
    "            # Export to CSV\n",
    "            csv_path_load = Path.cwd() / 'data' / 'single_pessimistic_pattern' / 'load_phase_results.csv'\n",
    "            load_phase_summary.to_csv(csv_path_load, index=False)\n",
    "            print(f\"✓ Load phase results exported: single_pessimistic_pattern/load_phase_results.csv ({len(load_phase_summary)} rows)\")\n",
    "\n",
    "            # Display summary\n",
    "            print(\"\\nLoad Phase Summary:\")\n",
    "            for phase in ['light', 'medium', 'heavy']:\n",
    "                phase_data = load_phase_summary[load_phase_summary['load_phase'] == phase]\n",
    "                if not phase_data.empty:\n",
    "                    total_requests = phase_data['count'].sum()\n",
    "                    avg_success_rate = phase_data['success_rate'].mean()\n",
    "                    avg_p95 = phase_data['p95_rt'].mean()\n",
    "                    print(f\"  {phase.capitalize()} phase: {total_requests} requests, Success rate: {avg_success_rate:.1f}%, P95 RT: {avg_p95:.1f}ms\")\n",
    "        else:\n",
    "            print(\"No load test results found in test_results.\")\n",
    "    else:\n",
    "        print(\"test_results not found or empty.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during load phase CSV export: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
